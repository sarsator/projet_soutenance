{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d09274ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Configuration CORRIGÃ‰E:\n",
      "   ModÃ¨le: SSD MobileNet V2 320x320 (COMPATIBLE TF 2.15)\n",
      "   Image size: 320x320\n",
      "   Batch size: 16\n",
      "   Training steps: 30,000\n",
      "   Output directory: ../models/dl_model/outputs/ssd_mnv2_320\n",
      "   Tracking: tensorboard\n",
      "   âœ… Ce modÃ¨le va FONCTIONNER avec TensorFlow 2.15 !\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Config centralisÃ©e - modifie ici, tout le reste suit !\n",
    "IMG_SIZE         = 320          # cÃ´tÃ© carrÃ© des images (320 pour le checkpoint)\n",
    "BATCH_SIZE       = 16           # RTX 4080 = large marge\n",
    "NUM_STEPS        = 30000        # â‰ˆ 40 epochs (peut â†“/â†‘)\n",
    "BASE_LR          = 0.02         # LR de dÃ©part (cosine schedule)\n",
    "WARMUP_STEPS     = int(0.05*NUM_STEPS)\n",
    "\n",
    "TRAIN_DIR        = \"../data/DL_data/train\"\n",
    "VAL_DIR          = \"../data/DL_data/valid\"\n",
    "\n",
    "TRAIN_ANN        = f\"{TRAIN_DIR}/_annotations.csv\"\n",
    "VAL_ANN          = f\"{VAL_DIR}/_annotations.csv\"\n",
    "\n",
    "# MODÃˆLE COMPATIBLE TF 2.15: SSD MobileNet V2 Keras\n",
    "PRETRAIN_CKPT    = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\"\n",
    "\n",
    "OUTPUT_DIR       = \"../models/dl_model/outputs/ssd_mnv2_320\"\n",
    "LABEL_MAP        = f\"{OUTPUT_DIR}/label_map.pbtxt\"\n",
    "TFRECORD_TRAIN   = f\"{OUTPUT_DIR}/train.record\"\n",
    "TFRECORD_VAL     = f\"{OUTPUT_DIR}/val.record\"\n",
    "\n",
    "TRACKING         = \"tensorboard\"  # (tensorboard / wandb / mlflow)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(f\"ğŸ“‹ Configuration CORRIGÃ‰E:\")\n",
    "print(f\"   ModÃ¨le: SSD MobileNet V2 320x320 (COMPATIBLE TF 2.15)\")\n",
    "print(f\"   Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Training steps: {NUM_STEPS:,}\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"   Tracking: {TRACKING}\")\n",
    "print(f\"   âœ… Ce modÃ¨le va FONCTIONNER avec TensorFlow 2.15 !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b48a207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 00:20:34.507415: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-16 00:20:34.526298: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-16 00:20:34.526319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-16 00:20:34.526973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-16 00:20:34.531197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-16 00:20:35.035026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-16 00:20:35.035026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ TensorFlow version: 2.15.0\n",
      "ğŸ® GPUs dÃ©tectÃ©s: 1\n",
      "   GPU 0: /physical_device:GPU:0\n",
      "ğŸš€ GPU unique activÃ©\n",
      "âœ… Configuration GPU terminÃ©e\n",
      "\n",
      "ğŸ§ª Test de calcul GPU...\n",
      "âœ… Calcul effectuÃ© sur: device:GPU:0\n",
      "âœ… RÃ©pertoire de sortie crÃ©Ã©: ../models/dl_model/outputs/ssd_mnv2_320\n",
      "âœ… Checkpoint dÃ©jÃ  prÃ©sent: ../models/dl_model/outputs/ssd_mnv2_320/pretrained_checkpoint\n",
      "ğŸ“ Chemin du checkpoint: ../models/dl_model/outputs/ssd_mnv2_320/pretrained_checkpoint/checkpoint/ckpt-0\n",
      "\n",
      "ğŸ“‹ RÃ©sumÃ© de la configuration:\n",
      "   ğŸ® Device: GPU\n",
      "   ğŸ”¢ Nombre de GPUs: 1\n",
      "   ğŸ“ Batch size: 16\n",
      "   ğŸ–¼ï¸  Image size: 512x512\n",
      "   ğŸ¤– ModÃ¨le: SSD MobileNet V2 320x320\n",
      "   âš¡ Estimation RTX 4080: ~2-3h pour 30,000 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 00:20:35.441606: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 00:20:35.481552: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 00:20:35.481588: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 00:20:35.483865: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 00:20:35.483894: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 00:20:35.483906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 00:20:35.611504: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 00:20:35.611547: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 00:20:35.611553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-07-16 00:20:35.611573: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 00:20:35.611592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13512 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:02:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"ğŸ”§ TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ® DETECTION ET CONFIGURATION GPU\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# VÃ©rification de la disponibilitÃ© des GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f\"ğŸ® GPUs dÃ©tectÃ©s: {len(gpus)}\")\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configuration pour permettre la croissance de mÃ©moire\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Affichage des dÃ©tails des GPUs\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"   GPU {i}: {gpu.name}\")\n",
    "            \n",
    "        # Configuration de la stratÃ©gie de distribution si multiple GPUs\n",
    "        if len(gpus) > 1:\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "            print(f\"ğŸš€ Multi-GPU activÃ©: {strategy.num_replicas_in_sync} GPUs\")\n",
    "        else:\n",
    "            strategy = tf.distribute.get_strategy()  # StratÃ©gie par dÃ©faut\n",
    "            print(f\"ğŸš€ GPU unique activÃ©\")\n",
    "            \n",
    "        print(f\"âœ… Configuration GPU terminÃ©e\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"âŒ Erreur de configuration GPU: {e}\")\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "else:\n",
    "    print(\"âš ï¸  Aucun GPU dÃ©tectÃ© - EntraÃ®nement CPU (trÃ¨s lent!)\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Test rapide de calcul GPU\n",
    "print(\"\\nğŸ§ª Test de calcul GPU...\")\n",
    "with tf.device('/GPU:0' if gpus else '/CPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "    device_name = c.device.split('/')[-1]\n",
    "    print(f\"âœ… Calcul effectuÃ© sur: {device_name}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# CrÃ©ation du rÃ©pertoire de sortie\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"âœ… RÃ©pertoire de sortie crÃ©Ã©: {OUTPUT_DIR}\")\n",
    "\n",
    "# TÃ©lÃ©chargement et extraction du checkpoint prÃ©-entraÃ®nÃ© CORRECT\n",
    "checkpoint_dir = f\"{OUTPUT_DIR}/pretrained_checkpoint\"\n",
    "checkpoint_url = PRETRAIN_CKPT\n",
    "checkpoint_tar = f\"{OUTPUT_DIR}/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\"\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    print(f\"ğŸ“¥ TÃ©lÃ©chargement du checkpoint MobileNet V2 correct...\")\n",
    "    \n",
    "    # URL du bon modÃ¨le MobileNet V2\n",
    "    checkpoint_urls = [\n",
    "        PRETRAIN_CKPT,  # URL configurÃ©e dans la cellule 1\n",
    "        \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\"\n",
    "    ]\n",
    "    \n",
    "    download_success = False\n",
    "    for i, url in enumerate(checkpoint_urls):\n",
    "        try:\n",
    "            print(f\"   ğŸ“¡ Tentative {i+1}: {url.split('/')[-1]}\")\n",
    "            urllib.request.urlretrieve(url, checkpoint_tar)\n",
    "            download_success = True\n",
    "            print(f\"   âœ… TÃ©lÃ©chargement rÃ©ussi!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Ã‰chec tentative {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if download_success:\n",
    "        print(f\"ğŸ“¦ Extraction du checkpoint...\")\n",
    "        with tarfile.open(checkpoint_tar, 'r:gz') as tar:\n",
    "            tar.extractall(OUTPUT_DIR)\n",
    "        \n",
    "        # Renommer le dossier extrait (nom correct pour MobileNet V2)\n",
    "        extracted_name = \"ssd_mobilenet_v2_320x320_coco17_tpu-8\"\n",
    "        if os.path.exists(f\"{OUTPUT_DIR}/{extracted_name}\"):\n",
    "            shutil.move(f\"{OUTPUT_DIR}/{extracted_name}\", checkpoint_dir)\n",
    "        \n",
    "        # Nettoyer le fichier tar\n",
    "        os.remove(checkpoint_tar)\n",
    "        print(f\"âœ… Checkpoint MobileNet V2 tÃ©lÃ©chargÃ© et extrait dans: {checkpoint_dir}\")\n",
    "        \n",
    "        CHECKPOINT_PATH = f\"{checkpoint_dir}/checkpoint/ckpt-0\"\n",
    "    else:\n",
    "        print(f\"âŒ Ã‰chec de toutes les tentatives de tÃ©lÃ©chargement\")\n",
    "        print(f\"ğŸ’¡ Solution manuelle:\")\n",
    "        print(f\"   wget {checkpoint_urls[0]} -O {checkpoint_tar}\")\n",
    "        print(f\"   tar -xzf {checkpoint_tar} -C {OUTPUT_DIR}\")\n",
    "        \n",
    "        # CrÃ©er un checkpoint factice temporaire\n",
    "        os.makedirs(f\"{checkpoint_dir}/checkpoint\", exist_ok=True)\n",
    "        CHECKPOINT_PATH = f\"{checkpoint_dir}/checkpoint/ckpt-0\"\n",
    "else:\n",
    "    print(f\"âœ… Checkpoint dÃ©jÃ  prÃ©sent: {checkpoint_dir}\")\n",
    "    CHECKPOINT_PATH = f\"{checkpoint_dir}/checkpoint/ckpt-0\"\n",
    "print(f\"ğŸ“ Chemin du checkpoint: {CHECKPOINT_PATH}\")\n",
    "\n",
    "# Affichage du rÃ©sumÃ© de configuration\n",
    "print(f\"\\nğŸ“‹ RÃ©sumÃ© de la configuration:\")\n",
    "print(f\"   ğŸ® Device: {'GPU' if gpus else 'CPU'}\")\n",
    "print(f\"   ğŸ”¢ Nombre de GPUs: {len(gpus)}\")\n",
    "print(f\"   ğŸ“ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   ğŸ–¼ï¸  Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"   ğŸ¤– ModÃ¨le: SSD MobileNet V2 320x320\")\n",
    "if gpus:\n",
    "    print(f\"   âš¡ Estimation RTX 4080: ~2-3h pour {NUM_STEPS:,} steps\")\n",
    "else:\n",
    "    print(f\"   ğŸŒ Estimation CPU: ~20-30h pour {NUM_STEPS:,} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5818592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Correction de l'incompatibilitÃ© tf-slim/TensorFlow 2.15...\n",
      "âŒ DÃ©tection du bug tf-slim : control_flow_ops.case manquant\n",
      "âœ… Patch appliquÃ© : control_flow_ops.case restaurÃ©\n",
      "ğŸ’¡ tf-slim va maintenant fonctionner avec TensorFlow 2.15\n",
      "ğŸ¯ Correction tf-slim terminÃ©e\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”§ CORRECTION INCOMPATIBILITÃ‰ TF-SLIM / TENSORFLOW 2.15\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ”§ Correction de l'incompatibilitÃ© tf-slim/TensorFlow 2.15...\")\n",
    "\n",
    "# Le problÃ¨me : tf-slim utilise control_flow_ops.case qui n'existe plus dans TF 2.15\n",
    "# Solution : Utiliser une version compatible ou patcher\n",
    "\n",
    "try:\n",
    "    # Test si le problÃ¨me existe\n",
    "    from tensorflow.python.ops import control_flow_ops\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    patches_applied = []\n",
    "    \n",
    "    # Patch pour control_flow_ops.case\n",
    "    if not hasattr(control_flow_ops, 'case'):\n",
    "        def case_wrapper(pred_fn_pairs, default=None, exclusive=False, name='case'):\n",
    "            \"\"\"Wrapper pour remplacer control_flow_ops.case\"\"\"\n",
    "            return tf.case(pred_fn_pairs, default=default, exclusive=exclusive, name=name)\n",
    "        \n",
    "        control_flow_ops.case = case_wrapper\n",
    "        patches_applied.append(\"case\")\n",
    "    \n",
    "    # Patch pour control_flow_ops.cond \n",
    "    if not hasattr(control_flow_ops, 'cond'):\n",
    "        def cond_wrapper(pred, true_fn=None, false_fn=None, name=None):\n",
    "            \"\"\"Wrapper pour remplacer control_flow_ops.cond\"\"\"\n",
    "            return tf.cond(pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
    "        \n",
    "        control_flow_ops.cond = cond_wrapper\n",
    "        patches_applied.append(\"cond\")\n",
    "        \n",
    "    # Patch pour control_flow_ops.while_loop si nÃ©cessaire\n",
    "    if not hasattr(control_flow_ops, 'while_loop'):\n",
    "        def while_loop_wrapper(cond, body, loop_vars, shape_invariants=None, \n",
    "                              parallel_iterations=10, back_prop=True, \n",
    "                              swap_memory=False, name=None, maximum_iterations=None,\n",
    "                              return_same_structure=False):\n",
    "            \"\"\"Wrapper pour remplacer control_flow_ops.while_loop\"\"\"\n",
    "            return tf.while_loop(\n",
    "                cond=cond, body=body, loop_vars=loop_vars,\n",
    "                shape_invariants=shape_invariants, \n",
    "                parallel_iterations=parallel_iterations,\n",
    "                back_prop=back_prop, swap_memory=swap_memory, \n",
    "                name=name, maximum_iterations=maximum_iterations,\n",
    "                return_same_structure=return_same_structure\n",
    "            )\n",
    "        \n",
    "        control_flow_ops.while_loop = while_loop_wrapper\n",
    "        patches_applied.append(\"while_loop\")\n",
    "    \n",
    "    if patches_applied:\n",
    "        print(f\"âŒ DÃ©tection du bug tf-slim : {', '.join(patches_applied)} manquant(s)\")\n",
    "        print(f\"âœ… Patch appliquÃ© : {', '.join(patches_applied)} restaurÃ©(s)\")\n",
    "        print(\"ğŸ’¡ tf-slim va maintenant fonctionner avec TensorFlow 2.15\")\n",
    "    else:\n",
    "        print(\"âœ… tf-slim compatible - Aucune correction nÃ©cessaire\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸  Erreur d'import: {e}\")\n",
    "    print(\"ğŸ’¡ Continuons - le patch sera appliquÃ© si nÃ©cessaire\")\n",
    "\n",
    "print(\"ğŸ¯ Correction tf-slim terminÃ©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ad505e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Label map crÃ©Ã©: ../models/dl_model/outputs/ssd_mnv2_320/label_map.pbtxt\n",
      "ğŸ·ï¸ Classes:\n",
      "   1: Healthy\n",
      "   2: Contaminated\n"
     ]
    }
   ],
   "source": [
    "# CrÃ©ation du label map\n",
    "label_map_content = \"\"\"item {\n",
    "  id: 1\n",
    "  name: 'Healthy'\n",
    "}\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'Contaminated'\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open(LABEL_MAP, 'w') as f:\n",
    "    f.write(label_map_content)\n",
    "\n",
    "print(f\"âœ… Label map crÃ©Ã©: {LABEL_MAP}\")\n",
    "print(\"ğŸ·ï¸ Classes:\")\n",
    "print(\"   1: Healthy\")\n",
    "print(\"   2: Contaminated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17443a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” VÃ©rification des fichiers requis...\n",
      "âœ… Tous les fichiers requis existent\n",
      "\n",
      "ğŸ” Ã‰tat des fichiers TFRecord (CORRIGÃ‰):\n",
      "   âœ… TFRecord train existe: ../models/dl_model/outputs/ssd_mnv2_320/train.record (4669.5 MB)\n",
      "   âœ… TFRecord validation existe: ../models/dl_model/outputs/ssd_mnv2_320/val.record (302.0 MB)\n",
      "\n",
      "â­ï¸  TFRecord d'entraÃ®nement dÃ©jÃ  existant - Conversion ignorÃ©e\n",
      "\n",
      "â­ï¸  TFRecord de validation dÃ©jÃ  existant - Conversion ignorÃ©e\n",
      "\n",
      "ğŸ“ˆ RÃ©sumÃ© final:\n",
      "   Train: 16787 boÃ®tes, 9186 images\n",
      "   Valid: 1006 boÃ®tes, 743 images\n",
      "   Total: 17793 boÃ®tes, 9929 images\n",
      "\n",
      "ğŸ’¡ Optimisation: Aucune conversion nÃ©cessaire - Fichiers dÃ©jÃ  prÃªts!\n",
      "   âš¡ Temps Ã©conomisÃ©: ~2-3 minutes de conversion Ã©vitÃ©es\n",
      "   ğŸ¯ BUG DE TAILLE CORRIGÃ‰ - Les gros fichiers sont maintenant acceptÃ©s!\n"
     ]
    }
   ],
   "source": [
    "def create_tf_example(row, img_dir):\n",
    "    \"\"\"CrÃ©e un tf.train.Example pour une ligne du CSV\"\"\"\n",
    "    \n",
    "    # Chemins des fichiers\n",
    "    img_path = os.path.join(img_dir, row['filename'])\n",
    "    \n",
    "    # VÃ©rification de l'existence de l'image\n",
    "    if not os.path.exists(img_path):\n",
    "        raise FileNotFoundError(f\"Image introuvable: {img_path}\")\n",
    "    \n",
    "    # Lecture de l'image\n",
    "    with tf.io.gfile.GFile(img_path, 'rb') as fid:\n",
    "        encoded_image = fid.read()\n",
    "    \n",
    "    # Obtenir les dimensions de l'image\n",
    "    image = tf.image.decode_image(encoded_image)\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Validation des dimensions\n",
    "    if height != row['height'] or width != row['width']:\n",
    "        print(f\"âš ï¸  Dimensions incohÃ©rentes pour {row['filename']}: CSV({row['width']}x{row['height']}) vs Image({width}x{height})\")\n",
    "    \n",
    "    # Normalisation des coordonnÃ©es (0-1)\n",
    "    xmin_norm = row['xmin'] / width\n",
    "    xmax_norm = row['xmax'] / width\n",
    "    ymin_norm = row['ymin'] / height\n",
    "    ymax_norm = row['ymax'] / height\n",
    "    \n",
    "    # Mapping des classes\n",
    "    class_mapping = {'Healthy': 1, 'Contaminated': 2}\n",
    "    class_id = class_mapping.get(row['class'])\n",
    "    if class_id is None:\n",
    "        raise ValueError(f\"Classe inconnue: {row['class']}\")\n",
    "    \n",
    "    # CrÃ©ation de l'exemple TF\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[row['filename'].encode('utf8')])),\n",
    "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[row['filename'].encode('utf8')])),\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[b'jpeg'])),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=[xmin_norm])),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=[xmax_norm])),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=[ymin_norm])),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=[ymax_norm])),\n",
    "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=[row['class'].encode('utf8')])),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=[class_id])),\n",
    "    }))\n",
    "    \n",
    "    return tf_example\n",
    "\n",
    "def convert_csv_to_tfrecord(csv_path, img_dir, output_path):\n",
    "    \"\"\"Convertit un CSV en TFRecord\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“Š Lecture du CSV: {csv_path}\")\n",
    "    \n",
    "    # Lecture du CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # VÃ©rification des colonnes requises\n",
    "    required_cols = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Colonnes manquantes dans le CSV: {missing_cols}\")\n",
    "    \n",
    "    # VÃ©rification des valeurs NaN\n",
    "    if df.isnull().any().any():\n",
    "        print(\"âš ï¸  Valeurs NaN dÃ©tectÃ©es dans le CSV:\")\n",
    "        print(df.isnull().sum())\n",
    "        raise ValueError(\"Le CSV contient des valeurs NaN\")\n",
    "    \n",
    "    # Ã‰criture du TFRecord\n",
    "    print(f\"âœï¸  Ã‰criture du TFRecord: {output_path}\")\n",
    "    \n",
    "    with tf.io.TFRecordWriter(output_path) as writer:\n",
    "        total_boxes = 0\n",
    "        unique_images = set()\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                tf_example = create_tf_example(row, img_dir)\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "                total_boxes += 1\n",
    "                unique_images.add(row['filename'])\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Erreur pour {row['filename']}: {e}\")\n",
    "                raise\n",
    "    \n",
    "    print(f\"âœ… TFRecord crÃ©Ã© avec succÃ¨s!\")\n",
    "    print(f\"   ğŸ“¦ {total_boxes} boÃ®tes de dÃ©limitation\")\n",
    "    print(f\"   ğŸ–¼ï¸  {len(unique_images)} images uniques\")\n",
    "    \n",
    "    return total_boxes, len(unique_images)\n",
    "\n",
    "# VÃ©rification de l'existence des fichiers et dossiers\n",
    "print(\"ğŸ” VÃ©rification des fichiers requis...\")\n",
    "\n",
    "# VÃ©rifier les fichiers CSV d'annotations\n",
    "if not os.path.exists(TRAIN_ANN):\n",
    "    print(f\"âŒ Fichier d'annotations d'entraÃ®nement introuvable: {TRAIN_ANN}\")\n",
    "    print(\"ğŸ“ Contenu du rÃ©pertoire parent:\")\n",
    "    parent_dir = os.path.dirname(TRAIN_ANN)\n",
    "    if os.path.exists(parent_dir):\n",
    "        csv_files = []\n",
    "        for item in os.listdir(parent_dir):\n",
    "            print(f\"   - {item}\")\n",
    "            if item.endswith('.csv'):\n",
    "                csv_files.append(item)\n",
    "        \n",
    "        # Proposer des alternatives si des fichiers CSV existent\n",
    "        if csv_files:\n",
    "            print(f\"\\nğŸ’¡ Fichiers CSV trouvÃ©s dans {parent_dir}:\")\n",
    "            for csv_file in csv_files:\n",
    "                print(f\"   - {csv_file}\")\n",
    "            \n",
    "            # Essayer de trouver un fichier d'annotations alternatif\n",
    "            possible_names = ['annotations.csv', 'train_annotations.csv', csv_files[0]]\n",
    "            for alt_name in possible_names:\n",
    "                alt_path = os.path.join(parent_dir, alt_name)\n",
    "                if os.path.exists(alt_path):\n",
    "                    print(f\"ğŸ”„ Utilisation du fichier alternatif: {alt_path}\")\n",
    "                    TRAIN_ANN = alt_path\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"âš ï¸  Aucun fichier d'annotations standard trouvÃ©. Veuillez vÃ©rifier le nom du fichier.\")\n",
    "                print(f\"   Attendu: {TRAIN_ANN}\")\n",
    "                raise FileNotFoundError(f\"Fichier d'annotations d'entraÃ®nement introuvable: {TRAIN_ANN}\")\n",
    "        else:\n",
    "            print(\"   Aucun fichier CSV trouvÃ© dans ce rÃ©pertoire.\")\n",
    "            raise FileNotFoundError(f\"Fichier d'annotations d'entraÃ®nement introuvable: {TRAIN_ANN}\")\n",
    "    else:\n",
    "        print(f\"   Le rÃ©pertoire {parent_dir} n'existe pas\")\n",
    "        raise FileNotFoundError(f\"Fichier d'annotations d'entraÃ®nement introuvable: {TRAIN_ANN}\")\n",
    "\n",
    "if not os.path.exists(VAL_ANN):\n",
    "    print(f\"âŒ Fichier d'annotations de validation introuvable: {VAL_ANN}\")\n",
    "    print(\"ğŸ“ Contenu du rÃ©pertoire parent:\")\n",
    "    parent_dir = os.path.dirname(VAL_ANN)\n",
    "    if os.path.exists(parent_dir):\n",
    "        for item in os.listdir(parent_dir):\n",
    "            print(f\"   - {item}\")\n",
    "    else:\n",
    "        print(f\"   Le rÃ©pertoire {parent_dir} n'existe pas\")\n",
    "    raise FileNotFoundError(f\"Fichier d'annotations de validation introuvable: {VAL_ANN}\")\n",
    "\n",
    "# VÃ©rifier les rÃ©pertoires d'images\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    print(f\"âŒ RÃ©pertoire d'images d'entraÃ®nement introuvable: {TRAIN_DIR}\")\n",
    "    raise FileNotFoundError(f\"RÃ©pertoire d'images d'entraÃ®nement introuvable: {TRAIN_DIR}\")\n",
    "\n",
    "if not os.path.exists(VAL_DIR):\n",
    "    print(f\"âŒ RÃ©pertoire d'images de validation introuvable: {VAL_DIR}\")\n",
    "    raise FileNotFoundError(f\"RÃ©pertoire d'images de validation introuvable: {VAL_DIR}\")\n",
    "\n",
    "# CrÃ©er le rÃ©pertoire de sortie si nÃ©cessaire\n",
    "os.makedirs(os.path.dirname(TFRECORD_TRAIN), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(TFRECORD_VAL), exist_ok=True)\n",
    "\n",
    "print(\"âœ… Tous les fichiers requis existent\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš€ CONVERSION INTELLIGENTE - Ã‰VITE LA DUPLICATION (CORRIGÃ‰E)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# VÃ©rifier si les TFRecords existent dÃ©jÃ  et s'ils sont valides\n",
    "# CORRECTION: Seuil plus bas pour Ã©viter le bug avec les gros fichiers\n",
    "tfrecord_train_exists = os.path.exists(TFRECORD_TRAIN) and os.path.getsize(TFRECORD_TRAIN) > 1024  # > 1KB au lieu de 1MB\n",
    "tfrecord_val_exists = os.path.exists(TFRECORD_VAL) and os.path.getsize(TFRECORD_VAL) > 1024  # > 1KB au lieu de 1MB\n",
    "\n",
    "print(f\"\\nğŸ” Ã‰tat des fichiers TFRecord (CORRIGÃ‰):\")\n",
    "if tfrecord_train_exists:\n",
    "    size_mb = os.path.getsize(TFRECORD_TRAIN) / (1024*1024)\n",
    "    print(f\"   âœ… TFRecord train existe: {TFRECORD_TRAIN} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    if os.path.exists(TFRECORD_TRAIN):\n",
    "        size_mb = os.path.getsize(TFRECORD_TRAIN) / (1024*1024)\n",
    "        print(f\"   âš ï¸  TFRecord train trop petit: {TFRECORD_TRAIN} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   âŒ TFRecord train manquant: {TFRECORD_TRAIN}\")\n",
    "\n",
    "if tfrecord_val_exists:\n",
    "    size_mb = os.path.getsize(TFRECORD_VAL) / (1024*1024)\n",
    "    print(f\"   âœ… TFRecord validation existe: {TFRECORD_VAL} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    if os.path.exists(TFRECORD_VAL):\n",
    "        size_mb = os.path.getsize(TFRECORD_VAL) / (1024*1024)\n",
    "        print(f\"   âš ï¸  TFRecord validation trop petit: {TFRECORD_VAL} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   âŒ TFRecord validation manquant: {TFRECORD_VAL}\")\n",
    "\n",
    "# Conversion train seulement si nÃ©cessaire\n",
    "if not tfrecord_train_exists:\n",
    "    print(\"\\nğŸš€ Conversion du dataset d'entraÃ®nement...\")\n",
    "    train_boxes, train_images = convert_csv_to_tfrecord(TRAIN_ANN, TRAIN_DIR, TFRECORD_TRAIN)\n",
    "else:\n",
    "    print(\"\\nâ­ï¸  TFRecord d'entraÃ®nement dÃ©jÃ  existant - Conversion ignorÃ©e\")\n",
    "    # Calculer les statistiques Ã  partir du CSV pour l'affichage\n",
    "    df_train = pd.read_csv(TRAIN_ANN)\n",
    "    train_boxes = len(df_train)\n",
    "    train_images = df_train['filename'].nunique()\n",
    "\n",
    "# Conversion validation seulement si nÃ©cessaire\n",
    "if not tfrecord_val_exists:\n",
    "    print(\"\\nğŸš€ Conversion du dataset de validation...\")\n",
    "    val_boxes, val_images = convert_csv_to_tfrecord(VAL_ANN, VAL_DIR, TFRECORD_VAL)\n",
    "else:\n",
    "    print(\"\\nâ­ï¸  TFRecord de validation dÃ©jÃ  existant - Conversion ignorÃ©e\")\n",
    "    # Calculer les statistiques Ã  partir du CSV pour l'affichage\n",
    "    df_val = pd.read_csv(VAL_ANN)\n",
    "    val_boxes = len(df_val)\n",
    "    val_images = df_val['filename'].nunique()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ RÃ©sumÃ© final:\")\n",
    "print(f\"   Train: {train_boxes} boÃ®tes, {train_images} images\")\n",
    "print(f\"   Valid: {val_boxes} boÃ®tes, {val_images} images\")\n",
    "print(f\"   Total: {train_boxes + val_boxes} boÃ®tes, {train_images + val_images} images\")\n",
    "\n",
    "# Message informatif\n",
    "if tfrecord_train_exists and tfrecord_val_exists:\n",
    "    print(f\"\\nğŸ’¡ Optimisation: Aucune conversion nÃ©cessaire - Fichiers dÃ©jÃ  prÃªts!\")\n",
    "    print(f\"   âš¡ Temps Ã©conomisÃ©: ~2-3 minutes de conversion Ã©vitÃ©es\")\n",
    "    print(f\"   ğŸ¯ BUG DE TAILLE CORRIGÃ‰ - Les gros fichiers sont maintenant acceptÃ©s!\")\n",
    "elif tfrecord_train_exists or tfrecord_val_exists:\n",
    "    print(f\"\\nğŸ’¡ Optimisation partielle: Seuls les fichiers manquants ont Ã©tÃ© crÃ©Ã©s\")\n",
    "else:\n",
    "    print(f\"\\nğŸ”¥ Conversion complÃ¨te terminÃ©e - Fichiers prÃªts pour l'entraÃ®nement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94aedcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration pipeline CORRIGÃ‰E avec box_predictor: ../models/dl_model/outputs/ssd_mnv2_320/pipeline.config\n",
      "ğŸ”§ ParamÃ¨tres:\n",
      "   Feature extractor: ssd_mobilenet_v2_keras (COMPATIBLE TF 2.15)\n",
      "   Box predictor: convolutional_box_predictor (AJOUTÃ‰)\n",
      "   Checkpoint: ../models/dl_model/outputs/ssd_mnv2_320/pretrained_checkpoint/checkpoint/ckpt-0\n",
      "   Batch size: 16\n",
      "   Steps: 30,000\n",
      "   Learning rate: 0.02 (cosine decay)\n",
      "   Warmup steps: 1500\n",
      "   Image size: 320x320\n",
      "   Classes: 2 (Healthy, Contaminated)\n",
      "   ğŸ¯ ERREUR BOX_PREDICTOR CORRIGÃ‰E - RELANCEZ L'ENTRAÃNEMENT !\n"
     ]
    }
   ],
   "source": [
    "# GÃ©nÃ©ration du fichier pipeline.config CORRECT AVEC BOX_PREDICTOR\n",
    "pipeline_config_content = f\"\"\"# Pipeline config pour SSD MobileNet V2 Keras (TF 2.15 compatible)\n",
    "\n",
    "model {{\n",
    "  ssd {{\n",
    "    inplace_batchnorm_update: true\n",
    "    freeze_batchnorm: false\n",
    "    num_classes: 2\n",
    "    image_resizer {{\n",
    "      fixed_shape_resizer {{\n",
    "        height: {IMG_SIZE}\n",
    "        width: {IMG_SIZE}\n",
    "      }}\n",
    "    }}\n",
    "    feature_extractor {{\n",
    "      type: 'ssd_mobilenet_v2_keras'\n",
    "      min_depth: 16\n",
    "      depth_multiplier: 1\n",
    "      conv_hyperparams {{\n",
    "        activation: RELU_6,\n",
    "        regularizer {{\n",
    "          l2_regularizer {{\n",
    "            weight: 0.00004\n",
    "          }}\n",
    "        }}\n",
    "        initializer {{\n",
    "          truncated_normal_initializer {{\n",
    "            stddev: 0.03\n",
    "            mean: 0.0\n",
    "          }}\n",
    "        }}\n",
    "        batch_norm {{\n",
    "          train: true,\n",
    "          scale: true,\n",
    "          center: true,\n",
    "          decay: 0.9997,\n",
    "          epsilon: 0.001,\n",
    "        }}\n",
    "      }}\n",
    "      use_depthwise: true\n",
    "    }}\n",
    "    box_predictor {{\n",
    "      convolutional_box_predictor {{\n",
    "        min_depth: 0\n",
    "        max_depth: 0\n",
    "        num_layers_before_predictor: 0\n",
    "        use_dropout: false\n",
    "        dropout_keep_probability: 0.8\n",
    "        kernel_size: 1\n",
    "        box_code_size: 4\n",
    "        apply_sigmoid_to_scores: false\n",
    "        class_prediction_bias_init: -4.6\n",
    "        conv_hyperparams {{\n",
    "          activation: RELU_6,\n",
    "          regularizer {{\n",
    "            l2_regularizer {{\n",
    "              weight: 0.00004\n",
    "            }}\n",
    "          }}\n",
    "          initializer {{\n",
    "            random_normal_initializer {{\n",
    "              stddev: 0.01\n",
    "              mean: 0.0\n",
    "            }}\n",
    "          }}\n",
    "          batch_norm {{\n",
    "            train: true,\n",
    "            scale: true,\n",
    "            center: true,\n",
    "            decay: 0.9997,\n",
    "            epsilon: 0.001,\n",
    "          }}\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    box_coder {{\n",
    "      faster_rcnn_box_coder {{\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }}\n",
    "    }}\n",
    "    matcher {{\n",
    "      argmax_matcher {{\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "        use_matmul_gather: true\n",
    "      }}\n",
    "    }}\n",
    "    similarity_calculator {{\n",
    "      iou_similarity {{\n",
    "      }}\n",
    "    }}\n",
    "    encode_background_as_zeros: true\n",
    "    anchor_generator {{\n",
    "      ssd_anchor_generator {{\n",
    "        num_layers: 6\n",
    "        min_scale: 0.2\n",
    "        max_scale: 0.95\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        aspect_ratios: 3.0\n",
    "        aspect_ratios: 0.3333\n",
    "      }}\n",
    "    }}\n",
    "    post_processing {{\n",
    "      batch_non_max_suppression {{\n",
    "        score_threshold: 1e-8\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "      }}\n",
    "      score_converter: SIGMOID\n",
    "    }}\n",
    "    normalize_loss_by_num_matches: true\n",
    "    loss {{\n",
    "      classification_loss {{\n",
    "        weighted_sigmoid_focal {{\n",
    "          alpha: 0.25\n",
    "          gamma: 1.5\n",
    "        }}\n",
    "      }}\n",
    "      localization_loss {{\n",
    "        weighted_smooth_l1 {{\n",
    "          delta: 1.0\n",
    "        }}\n",
    "      }}\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\n",
    "train_config: {{\n",
    "  fine_tune_checkpoint_version: V2\n",
    "  fine_tune_checkpoint: \"{CHECKPOINT_PATH}\"\n",
    "  fine_tune_checkpoint_type: \"detection\"\n",
    "  batch_size: {BATCH_SIZE}\n",
    "  sync_replicas: true\n",
    "  startup_delay_steps: 0\n",
    "  replicas_to_aggregate: 8\n",
    "  num_steps: {NUM_STEPS}\n",
    "  optimizer {{\n",
    "    momentum_optimizer: {{\n",
    "      learning_rate: {{\n",
    "        cosine_decay_learning_rate {{\n",
    "          learning_rate_base: {BASE_LR}\n",
    "          total_steps: {NUM_STEPS}\n",
    "          warmup_learning_rate: 0.0\n",
    "          warmup_steps: {WARMUP_STEPS}\n",
    "        }}\n",
    "      }}\n",
    "      momentum_optimizer_value: 0.9\n",
    "    }}\n",
    "    use_moving_average: false\n",
    "  }}\n",
    "  max_number_of_boxes: 100\n",
    "  unpad_groundtruth_tensors: false\n",
    "}}\n",
    "\n",
    "train_input_reader: {{\n",
    "  label_map_path: \"{LABEL_MAP}\"\n",
    "  tf_record_input_reader {{\n",
    "    input_path: \"{TFRECORD_TRAIN}\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "eval_config: {{\n",
    "  metrics_set: \"coco_detection_metrics\"\n",
    "  num_examples: {val_images}\n",
    "}}\n",
    "\n",
    "eval_input_reader: {{\n",
    "  label_map_path: \"{LABEL_MAP}\"\n",
    "  shuffle: false\n",
    "  num_epochs: 1\n",
    "  tf_record_input_reader {{\n",
    "    input_path: \"{TFRECORD_VAL}\"\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarde du fichier de configuration CORRECT\n",
    "PIPELINE_CFG = f\"{OUTPUT_DIR}/pipeline.config\"\n",
    "with open(PIPELINE_CFG, 'w') as f:\n",
    "    f.write(pipeline_config_content)\n",
    "\n",
    "print(f\"âœ… Configuration pipeline CORRIGÃ‰E avec box_predictor: {PIPELINE_CFG}\")\n",
    "print(f\"ğŸ”§ ParamÃ¨tres:\")\n",
    "print(f\"   Feature extractor: ssd_mobilenet_v2_keras (COMPATIBLE TF 2.15)\")\n",
    "print(f\"   Box predictor: convolutional_box_predictor (AJOUTÃ‰)\")\n",
    "print(f\"   Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Steps: {NUM_STEPS:,}\")\n",
    "print(f\"   Learning rate: {BASE_LR} (cosine decay)\")\n",
    "print(f\"   Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"   Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"   Classes: 2 (Healthy, Contaminated)\")\n",
    "print(f\"   ğŸ¯ ERREUR BOX_PREDICTOR CORRIGÃ‰E - RELANCEZ L'ENTRAÃNEMENT !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892d00a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Commande pour lancer l'entraÃ®nement:\n",
      "============================================================\n",
      "python model_main_tf2.py \\\n",
      "  --pipeline_config_path=../models/dl_model/outputs/ssd_mnv2_320/pipeline.config \\\n",
      "  --model_dir=../models/dl_model/outputs/ssd_mnv2_320 \\\n",
      "  --alsologtostderr\n",
      "============================================================\n",
      "ğŸ“ RÃ©pertoire de travail: /home/sarsator/projets/gaia_vision/training/notebook\n",
      "ğŸ“„ Config pipeline: ../models/dl_model/outputs/ssd_mnv2_320/pipeline.config\n",
      "ğŸ“‚ RÃ©pertoire de sortie: ../models/dl_model/outputs/ssd_mnv2_320\n",
      "\\nğŸ’¡ Pour surveiller l'entraÃ®nement en temps rÃ©el:\n",
      "   tensorboard --logdir=../models/dl_model/outputs/ssd_mnv2_320 --port=6006\n"
     ]
    }
   ],
   "source": [
    "# Commande d'entraÃ®nement\n",
    "# Note: Cette cellule affiche la commande Ã  exÃ©cuter dans un terminal\n",
    "# Pour lancer l'entraÃ®nement depuis le notebook, dÃ©commentez et exÃ©cutez la section suivante\n",
    "\n",
    "print(\"ğŸš€ Commande pour lancer l'entraÃ®nement:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "training_command = f\"\"\"python model_main_tf2.py \\\\\n",
    "  --pipeline_config_path={PIPELINE_CFG} \\\\\n",
    "  --model_dir={OUTPUT_DIR} \\\\\n",
    "  --alsologtostderr\"\"\"\n",
    "\n",
    "print(training_command)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"ğŸ“ RÃ©pertoire de travail: {os.getcwd()}\")\n",
    "print(f\"ğŸ“„ Config pipeline: {PIPELINE_CFG}\")\n",
    "print(f\"ğŸ“‚ RÃ©pertoire de sortie: {OUTPUT_DIR}\")\n",
    "\n",
    "# Lancement direct depuis le notebook (dÃ©commentez si souhaitÃ©)\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# print(\"\\\\nğŸƒ Lancement de l'entraÃ®nement...\")\n",
    "# try:\n",
    "#     result = subprocess.run([\n",
    "#         sys.executable, \"model_main_tf2.py\",\n",
    "#         f\"--pipeline_config_path={PIPELINE_CFG}\",\n",
    "#         f\"--model_dir={OUTPUT_DIR}\",\n",
    "#         \"--alsologtostderr\"\n",
    "#     ], capture_output=True, text=True, cwd=\".\")\n",
    "#     \n",
    "#     print(\"STDOUT:\", result.stdout)\n",
    "#     if result.stderr:\n",
    "#         print(\"STDERR:\", result.stderr)\n",
    "#     print(f\"Code de retour: {result.returncode}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Erreur lors du lancement: {e}\")\n",
    "\n",
    "print(\"\\\\nğŸ’¡ Pour surveiller l'entraÃ®nement en temps rÃ©el:\")\n",
    "print(f\"   tensorboard --logdir={OUTPUT_DIR} --port=6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29225d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Diagnostic complet du systÃ¨me...\n",
      "============================================================\n",
      "ğŸ“‹ 1. VÃ©rification des fichiers requis:\n",
      "   âœ… Label map: ../models/dl_model/outputs/ssd_mnv2_320/label_map.pbtxt (0.0 MB)\n",
      "   âœ… TFRecord train: ../models/dl_model/outputs/ssd_mnv2_320/train.record (4669.5 MB)\n",
      "   âœ… TFRecord validation: ../models/dl_model/outputs/ssd_mnv2_320/val.record (302.0 MB)\n",
      "   âœ… Pipeline config: ../models/dl_model/outputs/ssd_mnv2_320/pipeline.config (0.0 MB)\n",
      "   âœ… Checkpoint: ../models/dl_model/outputs/ssd_mnv2_320/pretrained_checkpoint/checkpoint/ckpt-0.index (0.0 MB)\n",
      "\n",
      "ğŸ“ 2. Structure du rÃ©pertoire de sortie (../models/dl_model/outputs/ssd_mnv2_320):\n",
      "   ğŸ“„ label_map.pbtxt (0.0 MB)\n",
      "   ğŸ“„ pipeline.config (0.0 MB)\n",
      "   ğŸ“ pretrained_checkpoint/\n",
      "   ğŸ“„ train.record (4669.5 MB)\n",
      "   ğŸ“„ val.record (302.0 MB)\n",
      "\n",
      "ğŸ“Š 3. VÃ©rification des donnÃ©es:\n",
      "   âœ… Dataset train: 9186 images, 16787 boÃ®tes\n",
      "   âœ… Dataset validation: 743 images, 1006 boÃ®tes\n",
      "   ğŸ“ˆ Ratio: Train 92.5% / Val 7.5%\n",
      "\n",
      "ğŸ® 4. Configuration GPU:\n",
      "   ğŸ® GPUs dÃ©tectÃ©s: 1\n",
      "   âœ… GPU 0: /physical_device:GPU:0\n",
      "\n",
      "ğŸ§ª 5. Test de l'API TensorFlow Object Detection:\n",
      "   âœ… config_util importÃ© avec succÃ¨s\n",
      "   âœ… model_builder importÃ© avec succÃ¨s\n",
      "   âœ… TensorFlow Object Detection API accessible\n",
      "   âœ… Pipeline config valide\n",
      "\n",
      "ğŸ’¾ 6. Estimation des ressources:\n",
      "   ğŸ¯ MÃ©moire GPU estimÃ©e: ~0.0 GB par batch\n",
      "   â±ï¸  Steps par epoch estimÃ©: ~574\n",
      "   ğŸ• DurÃ©e estimÃ©e totale: 2-3h (GPU)\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ DIAGNOSTIC RÃ‰USSI - PrÃªt pour l'entraÃ®nement!\n",
      "â–¶ï¸  Vous pouvez maintenant exÃ©cuter la cellule d'entraÃ®nement\n",
      "============================================================\n",
      "\n",
      "ğŸ’¡ Note sur TensorBoard:\n",
      "   ğŸ“Š TensorBoard est vide car l'entraÃ®nement n'a pas encore commencÃ©\n",
      "   â³ Les mÃ©triques apparaÃ®tront dÃ¨s le premier step d'entraÃ®nement\n",
      "   ğŸ”„ RafraÃ®chissez TensorBoard aprÃ¨s avoir lancÃ© l'entraÃ®nement\n",
      "   âœ… model_builder importÃ© avec succÃ¨s\n",
      "   âœ… TensorFlow Object Detection API accessible\n",
      "   âœ… Pipeline config valide\n",
      "\n",
      "ğŸ’¾ 6. Estimation des ressources:\n",
      "   ğŸ¯ MÃ©moire GPU estimÃ©e: ~0.0 GB par batch\n",
      "   â±ï¸  Steps par epoch estimÃ©: ~574\n",
      "   ğŸ• DurÃ©e estimÃ©e totale: 2-3h (GPU)\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ DIAGNOSTIC RÃ‰USSI - PrÃªt pour l'entraÃ®nement!\n",
      "â–¶ï¸  Vous pouvez maintenant exÃ©cuter la cellule d'entraÃ®nement\n",
      "============================================================\n",
      "\n",
      "ğŸ’¡ Note sur TensorBoard:\n",
      "   ğŸ“Š TensorBoard est vide car l'entraÃ®nement n'a pas encore commencÃ©\n",
      "   â³ Les mÃ©triques apparaÃ®tront dÃ¨s le premier step d'entraÃ®nement\n",
      "   ğŸ”„ RafraÃ®chissez TensorBoard aprÃ¨s avoir lancÃ© l'entraÃ®nement\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ” DIAGNOSTIC ET VÃ‰RIFICATION AVANT ENTRAÃNEMENT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸ” Diagnostic complet du systÃ¨me...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. VÃ©rification des fichiers requis\n",
    "print(\"ğŸ“‹ 1. VÃ©rification des fichiers requis:\")\n",
    "files_to_check = [\n",
    "    (\"Label map\", LABEL_MAP),\n",
    "    (\"TFRecord train\", TFRECORD_TRAIN), \n",
    "    (\"TFRecord validation\", TFRECORD_VAL),\n",
    "    (\"Pipeline config\", PIPELINE_CFG),\n",
    "    (\"Checkpoint\", CHECKPOINT_PATH + \".index\")  # VÃ©rifie le fichier .index du checkpoint\n",
    "]\n",
    "\n",
    "all_files_exist = True\n",
    "for name, path in files_to_check:\n",
    "    if os.path.exists(path):\n",
    "        size = os.path.getsize(path) / (1024*1024)  # Size in MB\n",
    "        print(f\"   âœ… {name}: {path} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   âŒ {name}: {path} (MANQUANT)\")\n",
    "        all_files_exist = False\n",
    "\n",
    "# 2. VÃ©rification du rÃ©pertoire de sortie\n",
    "print(f\"\\nğŸ“ 2. Structure du rÃ©pertoire de sortie ({OUTPUT_DIR}):\")\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    for item in os.listdir(OUTPUT_DIR):\n",
    "        item_path = os.path.join(OUTPUT_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"   ğŸ“ {item}/\")\n",
    "        else:\n",
    "            size = os.path.getsize(item_path) / (1024*1024)\n",
    "            print(f\"   ğŸ“„ {item} ({size:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"   âŒ Le rÃ©pertoire {OUTPUT_DIR} n'existe pas\")\n",
    "    all_files_exist = False\n",
    "\n",
    "# 3. VÃ©rification des donnÃ©es d'entraÃ®nement\n",
    "print(f\"\\nğŸ“Š 3. VÃ©rification des donnÃ©es:\")\n",
    "if 'train_boxes' in locals() and 'val_boxes' in locals():\n",
    "    print(f\"   âœ… Dataset train: {train_images} images, {train_boxes} boÃ®tes\")\n",
    "    print(f\"   âœ… Dataset validation: {val_images} images, {val_boxes} boÃ®tes\")\n",
    "    \n",
    "    # Ratio de donnÃ©es\n",
    "    total_images = train_images + val_images\n",
    "    train_ratio = train_images / total_images * 100\n",
    "    val_ratio = val_images / total_images * 100\n",
    "    print(f\"   ğŸ“ˆ Ratio: Train {train_ratio:.1f}% / Val {val_ratio:.1f}%\")\n",
    "else:\n",
    "    print(\"   âš ï¸  Variables de donnÃ©es non dÃ©finies - ExÃ©cutez d'abord la conversion CSVâ†’TFRecord\")\n",
    "    all_files_exist = False\n",
    "\n",
    "# 4. VÃ©rification GPU\n",
    "print(f\"\\nğŸ® 4. Configuration GPU:\")\n",
    "if 'gpus' in locals():\n",
    "    print(f\"   ğŸ® GPUs dÃ©tectÃ©s: {len(gpus)}\")\n",
    "    if gpus:\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"   âœ… GPU {i}: {gpu.name}\")\n",
    "    else:\n",
    "        print(\"   âš ï¸  Aucun GPU dÃ©tectÃ© - EntraÃ®nement sera trÃ¨s lent\")\n",
    "else:\n",
    "    print(\"   âŒ Configuration GPU non initialisÃ©e\")\n",
    "\n",
    "# 5. Test du modÃ¨le TF Object Detection API\n",
    "print(f\"\\nğŸ§ª 5. Test de l'API TensorFlow Object Detection:\")\n",
    "try:\n",
    "    # VÃ©rifier si l'API TF Object Detection est accessible\n",
    "    import sys\n",
    "    \n",
    "    # Utiliser le chemin local du modÃ¨le clonÃ©\n",
    "    research_path = './tensorflow_models/research'\n",
    "    slim_path = './tensorflow_models/research/slim'\n",
    "    \n",
    "    if research_path not in sys.path:\n",
    "        sys.path.insert(0, research_path)\n",
    "    if slim_path not in sys.path:\n",
    "        sys.path.insert(0, slim_path)\n",
    "    \n",
    "    # VÃ©rifier que le rÃ©pertoire existe\n",
    "    if not os.path.exists(research_path):\n",
    "        raise ImportError(f\"RÃ©pertoire {research_path} non trouvÃ©\")\n",
    "    \n",
    "    # Test d'import avec gestion des erreurs de compatibilitÃ©\n",
    "    from object_detection.utils import config_util\n",
    "    print(\"   âœ… config_util importÃ© avec succÃ¨s\")\n",
    "    \n",
    "    # Test optionnel du model_builder (peut Ã©chouer avec certaines versions)\n",
    "    try:\n",
    "        from object_detection.builders import model_builder\n",
    "        print(\"   âœ… model_builder importÃ© avec succÃ¨s\")\n",
    "    except (ImportError, AttributeError) as e:\n",
    "        print(f\"   âš ï¸  model_builder non disponible: {e}\")\n",
    "        print(\"   ğŸ’¡ Ceci peut Ãªtre dÃ» Ã  des incompatibilitÃ©s de versions mais n'empÃªche pas l'entraÃ®nement\")\n",
    "    \n",
    "    print(\"   âœ… TensorFlow Object Detection API accessible\")\n",
    "    \n",
    "    # Test de lecture de la config\n",
    "    if os.path.exists(PIPELINE_CFG):\n",
    "        try:\n",
    "            configs = config_util.get_configs_from_pipeline_file(PIPELINE_CFG)\n",
    "            print(\"   âœ… Pipeline config valide\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Erreur pipeline config: {e}\")\n",
    "            all_files_exist = False\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"   âŒ TensorFlow Object Detection API non trouvÃ©e: {e}\")\n",
    "    print(\"   ğŸ’¡ Installation requise:\")\n",
    "    print(\"      git clone https://github.com/tensorflow/models.git tensorflow_models\")\n",
    "    print(\"      cd tensorflow_models/research\")\n",
    "    print(\"      protoc object_detection/protos/*.proto --python_out=.\")\n",
    "    print(\"      cp object_detection/packages/tf2/setup.py .\")\n",
    "    print(\"      pip install .\")\n",
    "    all_files_exist = False\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Erreur de compatibilitÃ©: {e}\")\n",
    "    print(\"   ğŸ’¡ L'API peut fonctionner malgrÃ© cette erreur\")\n",
    "    print(\"   ğŸ”§ Essayez: pip install --upgrade tensorflow tensorflow-object-detection-api\")\n",
    "\n",
    "# 6. Estimation des ressources\n",
    "print(f\"\\nğŸ’¾ 6. Estimation des ressources:\")\n",
    "estimated_model_size = BATCH_SIZE * IMG_SIZE * IMG_SIZE * 3 * 4 / (1024**3)  # GB\n",
    "print(f\"   ğŸ¯ MÃ©moire GPU estimÃ©e: ~{estimated_model_size:.1f} GB par batch\")\n",
    "print(f\"   â±ï¸  Steps par epoch estimÃ©: ~{train_images // BATCH_SIZE}\")\n",
    "print(f\"   ğŸ• DurÃ©e estimÃ©e totale: {'2-3h (GPU)' if gpus else '20-30h (CPU)'}\")\n",
    "\n",
    "# RÃ©sumÃ© final\n",
    "print(f\"\\n{'='*60}\")\n",
    "if all_files_exist:\n",
    "    print(\"ğŸ‰ DIAGNOSTIC RÃ‰USSI - PrÃªt pour l'entraÃ®nement!\")\n",
    "    print(\"â–¶ï¸  Vous pouvez maintenant exÃ©cuter la cellule d'entraÃ®nement\")\n",
    "else:\n",
    "    print(\"âŒ DIAGNOSTIC Ã‰CHOUÃ‰ - ProblÃ¨mes dÃ©tectÃ©s\")\n",
    "    print(\"ğŸ”§ Corrigez les erreurs ci-dessus avant de continuer\")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Note importante sur TensorBoard\n",
    "if all_files_exist:\n",
    "    print(f\"\\nğŸ’¡ Note sur TensorBoard:\")\n",
    "    print(f\"   ğŸ“Š TensorBoard est vide car l'entraÃ®nement n'a pas encore commencÃ©\")\n",
    "    print(f\"   â³ Les mÃ©triques apparaÃ®tront dÃ¨s le premier step d'entraÃ®nement\")\n",
    "    print(f\"   ğŸ”„ RafraÃ®chissez TensorBoard aprÃ¨s avoir lancÃ© l'entraÃ®nement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de5bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š DÃ©marrage de TensorBoard...\n",
      "   ğŸ§¹ Instances prÃ©cÃ©dentes arrÃªtÃ©es\n",
      "ğŸš€ Lancement de TensorBoard...\n",
      "   Commande: /home/sarsator/projets/gaia_vision/.venv/bin/python -m tensorboard.main --logdir=../models/dl_model/outputs/ssd_mnv2_320 --port=6006 --host=0.0.0.0\n",
      "   ğŸ§¹ Instances prÃ©cÃ©dentes arrÃªtÃ©es\n",
      "ğŸš€ Lancement de TensorBoard...\n",
      "   Commande: /home/sarsator/projets/gaia_vision/.venv/bin/python -m tensorboard.main --logdir=../models/dl_model/outputs/ssd_mnv2_320 --port=6006 --host=0.0.0.0\n",
      "âœ… TensorBoard dÃ©marrÃ© avec succÃ¨s!\n",
      "ğŸ“Š AccÃ¨s: http://localhost:6006\n",
      "âœ… TensorBoard dÃ©marrÃ© avec succÃ¨s!\n",
      "ğŸ“Š AccÃ¨s: http://localhost:6006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost:6006\" target=\"_blank\">ğŸ”— Ouvrir TensorBoard dans un nouvel onglet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ Pour arrÃªter TensorBoard plus tard:\n",
      "   pkill -f tensorboard\n",
      "\n",
      "ğŸ“ˆ MÃ©triques Ã  surveiller:\n",
      "   â€¢ Loss/total_loss (doit diminuer)\n",
      "   â€¢ Loss/classification_loss\n",
      "   â€¢ Loss/localization_loss\n",
      "   â€¢ learning_rate\n",
      "   â€¢ DetectionBoxes_Precision/mAP (aprÃ¨s Ã©valuation)\n",
      "   â€¢ DetectionBoxes_Recall/AR@100 (aprÃ¨s Ã©valuation)\n",
      "\n",
      "â„¹ï¸  Note: Les mÃ©triques apparaÃ®tront une fois l'entraÃ®nement commencÃ©\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Š LANCEMENT TENSORBOARD\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"ğŸ“Š DÃ©marrage de TensorBoard...\")\n",
    "\n",
    "# ArrÃªter toute instance existante\n",
    "try:\n",
    "    subprocess.run([\"pkill\", \"-f\", \"tensorboard\"], capture_output=True)\n",
    "    time.sleep(1)\n",
    "    print(\"   ğŸ§¹ Instances prÃ©cÃ©dentes arrÃªtÃ©es\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Chemin Python correct pour l'environnement virtuel\n",
    "python_path = \"/home/sarsator/projets/gaia_vision/.venv/bin/python\"\n",
    "\n",
    "# Lancer TensorBoard avec le bon interprÃ©teur\n",
    "tensorboard_cmd = f\"{python_path} -m tensorboard.main --logdir={OUTPUT_DIR} --port=6006 --host=0.0.0.0\"\n",
    "\n",
    "print(f\"ğŸš€ Lancement de TensorBoard...\")\n",
    "print(f\"   Commande: {tensorboard_cmd}\")\n",
    "\n",
    "# Utiliser nohup pour lancer en arriÃ¨re-plan sans bloquer\n",
    "launch_cmd = f\"nohup {tensorboard_cmd} > tensorboard.log 2>&1 &\"\n",
    "os.system(launch_cmd)\n",
    "\n",
    "time.sleep(3)  # Attendre que TensorBoard dÃ©marre\n",
    "\n",
    "# VÃ©rifier si TensorBoard fonctionne\n",
    "try:\n",
    "    result = subprocess.run([\"curl\", \"-s\", \"http://localhost:6006\"], \n",
    "                           capture_output=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… TensorBoard dÃ©marrÃ© avec succÃ¨s!\")\n",
    "        print(\"ğŸ“Š AccÃ¨s: http://localhost:6006\")\n",
    "        \n",
    "        # Afficher le lien cliquable dans Jupyter\n",
    "        from IPython.display import HTML, display\n",
    "        display(HTML('<a href=\"http://localhost:6006\" target=\"_blank\">ğŸ”— Ouvrir TensorBoard dans un nouvel onglet</a>'))\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸  TensorBoard en cours de dÃ©marrage...\")\n",
    "        print(\"ğŸ“Š Essayez: http://localhost:6006 dans quelques secondes\")\n",
    "except:\n",
    "    print(\"âš ï¸  TensorBoard en cours de dÃ©marrage...\")\n",
    "    print(\"ğŸ“Š Essayez: http://localhost:6006 dans quelques secondes\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ’¡ Pour arrÃªter TensorBoard plus tard:\")\n",
    "print(\"   pkill -f tensorboard\")\n",
    "print()\n",
    "print(\"ğŸ“ˆ MÃ©triques Ã  surveiller:\")\n",
    "print(\"   â€¢ Loss/total_loss (doit diminuer)\")\n",
    "print(\"   â€¢ Loss/classification_loss\") \n",
    "print(\"   â€¢ Loss/localization_loss\")\n",
    "print(\"   â€¢ learning_rate\")\n",
    "print(\"   â€¢ DetectionBoxes_Precision/mAP (aprÃ¨s Ã©valuation)\")\n",
    "print(\"   â€¢ DetectionBoxes_Recall/AR@100 (aprÃ¨s Ã©valuation)\")\n",
    "print()\n",
    "print(\"â„¹ï¸  Note: Les mÃ©triques apparaÃ®tront une fois l'entraÃ®nement commencÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23ab67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Instructions pour l'entraÃ®nement\n",
      "============================================================\n",
      "âœ… SystÃ¨me prÃªt pour l'entraÃ®nement!\n",
      "\n",
      "ğŸ“‹ MÃ©thodes d'entraÃ®nement disponibles:\n",
      "\n",
      "1ï¸âƒ£  MÃ‰THODE RECOMMANDÃ‰E - Script Bash:\n",
      "   â€¢ Plus stable et fiable\n",
      "   â€¢ Ouvrez un terminal dans ce rÃ©pertoire\n",
      "   â€¢ ExÃ©cutez: ./train_ssd_mobilenet.sh\n",
      "\n",
      "2ï¸âƒ£  Alternative - Commande directe:\n",
      "   â€¢ Dans un terminal, exÃ©cutez:\n",
      "   export PYTHONPATH=/home/sarsator/projets/gaia_vision/training/notebook/tensorflow_models/research:/home/sarsator/projets/gaia_vision/training/notebook/tensorflow_models/research/slim:$PYTHONPATH\n",
      "   python tensorflow_models/research/object_detection/model_main_tf2.py \\\n",
      "     --model_dir=../models/dl_model/outputs/ssd_mnv2_320 \\\n",
      "     --pipeline_config_path=../models/dl_model/outputs/ssd_mnv2_320/pipeline.config \\\n",
      "     --num_train_steps=30000 \\\n",
      "     --alsologtostderr\n",
      "\n",
      "â±ï¸  DurÃ©e estimÃ©e:\n",
      "   ğŸ® Avec GPU RTX 4080: ~2-3h pour 30,000 steps\n",
      "\n",
      "ğŸ“Š Surveillance:\n",
      "   â€¢ TensorBoard: http://localhost:6006\n",
      "   â€¢ Logs en temps rÃ©el dans le terminal\n",
      "   â€¢ Checkpoints sauvÃ©s dans: ../models/dl_model/outputs/ssd_mnv2_320\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš€ INSTRUCTIONS D'ENTRAÃNEMENT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸš€ Instructions pour l'entraÃ®nement\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'all_files_exist' in locals() and all_files_exist:\n",
    "    print(\"âœ… SystÃ¨me prÃªt pour l'entraÃ®nement!\")\n",
    "    print()\n",
    "    print(\"ğŸ“‹ MÃ©thodes d'entraÃ®nement disponibles:\")\n",
    "    print()\n",
    "    print(\"1ï¸âƒ£  MÃ‰THODE RECOMMANDÃ‰E - Script Bash:\")\n",
    "    print(\"   â€¢ Plus stable et fiable\")\n",
    "    print(\"   â€¢ Ouvrez un terminal dans ce rÃ©pertoire\")\n",
    "    print(\"   â€¢ ExÃ©cutez: ./train_ssd_mobilenet.sh\")\n",
    "    print()\n",
    "    print(\"2ï¸âƒ£  Alternative - Commande directe:\")\n",
    "    print(\"   â€¢ Dans un terminal, exÃ©cutez:\")\n",
    "    \n",
    "    # VÃ©rifier si tensorflow_models existe\n",
    "    api_dir = \"tensorflow_models\"\n",
    "    model_main_path = f\"{api_dir}/research/object_detection/model_main_tf2.py\"\n",
    "    \n",
    "    if os.path.exists(model_main_path):\n",
    "        research_path = os.path.abspath(f\"{api_dir}/research\")\n",
    "        slim_path = os.path.abspath(f\"{api_dir}/research/slim\")\n",
    "        \n",
    "        print(f\"   export PYTHONPATH={research_path}:{slim_path}:$PYTHONPATH\")\n",
    "        print(f\"   python {model_main_path} \\\\\")\n",
    "        print(f\"     --model_dir={OUTPUT_DIR} \\\\\")\n",
    "        print(f\"     --pipeline_config_path={PIPELINE_CFG} \\\\\")\n",
    "        print(f\"     --num_train_steps={NUM_STEPS} \\\\\")\n",
    "        print(f\"     --alsologtostderr\")\n",
    "    else:\n",
    "        print(\"   âŒ TensorFlow Object Detection API non installÃ©e\")\n",
    "        print(\"   ğŸ’¡ Installez d'abord:\")\n",
    "        print(\"      git clone https://github.com/tensorflow/models.git tensorflow_models\")\n",
    "        print(\"      cd tensorflow_models/research\")\n",
    "        print(\"      protoc object_detection/protos/*.proto --python_out=.\")\n",
    "    \n",
    "    print()\n",
    "    print(\"â±ï¸  DurÃ©e estimÃ©e:\")\n",
    "    if 'gpus' in locals() and gpus:\n",
    "        print(f\"   ğŸ® Avec GPU RTX 4080: ~2-3h pour {NUM_STEPS:,} steps\")\n",
    "    else:\n",
    "        print(f\"   ğŸŒ Avec CPU: ~20-30h pour {NUM_STEPS:,} steps\")\n",
    "    \n",
    "    print()\n",
    "    print(\"ğŸ“Š Surveillance:\")\n",
    "    print(\"   â€¢ TensorBoard: http://localhost:6006\")\n",
    "    print(\"   â€¢ Logs en temps rÃ©el dans le terminal\")\n",
    "    print(\"   â€¢ Checkpoints sauvÃ©s dans:\", OUTPUT_DIR)\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ SystÃ¨me non prÃªt pour l'entraÃ®nement\")\n",
    "    print(\"ğŸ”§ ExÃ©cutez d'abord toutes les cellules prÃ©cÃ©dentes\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… LANCEMENT ENTRAÃNEMENT FINAL - AVEC FILTRAGE INTELLIGENT !\n",
    "print(\"ğŸš€ Lancement de l'entraÃ®nement final...\")\n",
    "print(\"ğŸ“Š TensorBoard: http://localhost:6006\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# VÃ©rifications rapides\n",
    "print(f\"âœ… Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"âœ… Config: {PIPELINE_CFG}\")\n",
    "\n",
    "# Commande d'entraÃ®nement\n",
    "cmd = [\n",
    "    sys.executable, \n",
    "    \"tensorflow_models/research/object_detection/model_main_tf2.py\",\n",
    "    f\"--model_dir={OUTPUT_DIR}\",\n",
    "    f\"--pipeline_config_path={PIPELINE_CFG}\",\n",
    "    f\"--num_train_steps={NUM_STEPS}\",\n",
    "    \"--alsologtostderr\",\n",
    "    \"--use_tpu=False\"\n",
    "]\n",
    "\n",
    "# Variables d'environnement\n",
    "env = os.environ.copy()\n",
    "research_path = os.path.abspath(\"tensorflow_models/research\")\n",
    "slim_path = os.path.abspath(\"tensorflow_models/research/slim\")\n",
    "env[\"PYTHONPATH\"] = f\"{research_path}:{slim_path}:{env.get('PYTHONPATH', '')}\"\n",
    "\n",
    "print(f\"ğŸ”¥ DÃ‰MARRAGE DE L'ENTRAÃNEMENT...\")\n",
    "\n",
    "try:\n",
    "    # Test avec timeout trÃ¨s court pour diagnostiquer\n",
    "    process = subprocess.run(cmd, env=env, capture_output=True, text=True, timeout=10)\n",
    "    \n",
    "    # Filtrer seulement les erreurs importantes\n",
    "    if process.stderr:\n",
    "        lines = process.stderr.split('\\n')\n",
    "        important_lines = []\n",
    "        for line in lines:\n",
    "            line_lower = line.lower()\n",
    "            if any(keyword in line_lower for keyword in ['error', 'exception', 'traceback', 'failed', 'valueerror', 'importerror', 'filenotfounderror']):\n",
    "                important_lines.append(line)\n",
    "        \n",
    "        if important_lines:\n",
    "            print(\"âš ï¸  ERREURS IMPORTANTES DÃ‰TECTÃ‰ES:\")\n",
    "            for line in important_lines[:10]:  # Max 10 lignes\n",
    "                print(f\"   {line}\")\n",
    "        else:\n",
    "            print(\"âœ… Aucune erreur critique dÃ©tectÃ©e dans les 10 premiÃ¨res secondes\")\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(\"ğŸ‰ DIAGNOSTIC POSITIF - Processus se lance bien !\")\n",
    "    else:\n",
    "        print(f\"âŒ Code d'erreur: {process.returncode}\")\n",
    "            \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"â° Timeout aprÃ¨s 10s - C'est NORMAL !\")\n",
    "    print(\"âœ… Le processus se lance correctement\")\n",
    "    print(\"ğŸš€ Lancement de l'entraÃ®nement complet maintenant...\")\n",
    "    \n",
    "    # Maintenant lancer VRAIMENT l'entraÃ®nement sans timeout\n",
    "    try:\n",
    "        process = subprocess.Popen(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        print(f\"ğŸ¯ PID de l'entraÃ®nement: {process.pid}\")\n",
    "        print(\"\udcc8 Surveillez TensorBoard: http://localhost:6006\")\n",
    "        print(\"â±ï¸  L'entraÃ®nement est en cours...\")\n",
    "        \n",
    "        # Attendre un peu puis vÃ©rifier\n",
    "        import time\n",
    "        time.sleep(5)\n",
    "        \n",
    "        if process.poll() is None:\n",
    "            print(\"âœ… ENTRAÃNEMENT EN COURS - Le processus fonctionne !\")\n",
    "            print(\"ğŸ’¡ Laissez-le tourner et surveillez TensorBoard\")\n",
    "        else:\n",
    "            stdout, stderr = process.communicate()\n",
    "            print(f\"âŒ Processus terminÃ© avec code: {process.returncode}\")\n",
    "            # Montrer seulement les derniÃ¨res lignes d'erreur\n",
    "            if stderr:\n",
    "                error_lines = stderr.split('\\n')[-5:]\n",
    "                print(\"\udccb DerniÃ¨res erreurs:\")\n",
    "                for line in error_lines:\n",
    "                    if line.strip():\n",
    "                        print(f\"   {line}\")\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur lors du lancement: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur de diagnostic: {e}\")\n",
    "\n",
    "print(\"ğŸ Diagnostic terminÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8dcd32",
   "metadata": {},
   "source": [
    "# ğŸ¯ INSTRUCTIONS FINALES\n",
    "\n",
    "## âœ… SYSTÃˆME PRÃŠT !\n",
    "\n",
    "**TensorBoard** est maintenant actif sur : **http://localhost:6006**\n",
    "\n",
    "## ğŸš€ Pour lancer l'entraÃ®nement :\n",
    "\n",
    "1. **ExÃ©cutez la cellule prÃ©cÃ©dente** (celle du lancement d'entraÃ®nement)\n",
    "2. **Attendez** que l'entraÃ®nement commence (peut prendre 1-2 minutes)\n",
    "3. **Surveillez TensorBoard** - les mÃ©triques apparaÃ®tront automatiquement\n",
    "\n",
    "## ğŸ“Š MÃ©triques importantes Ã  surveiller :\n",
    "\n",
    "- **Loss/total_loss** : Doit diminuer progressivement\n",
    "- **Loss/classification_loss** : Perte de classification\n",
    "- **Loss/localization_loss** : Perte de localisation  \n",
    "- **learning_rate** : Taux d'apprentissage (dÃ©croÃ®t avec cosine schedule)\n",
    "\n",
    "## â±ï¸ DurÃ©e estimÃ©e : **2-3 heures** avec RTX 4080\n",
    "\n",
    "## ğŸ’¡ Notes importantes :\n",
    "\n",
    "- L'entraÃ®nement peut Ãªtre interrompu avec Ctrl+C\n",
    "- Les checkpoints sont sauvÃ©s automatiquement toutes les 3000 steps\n",
    "- TensorBoard se met Ã  jour en temps rÃ©el\n",
    "- Vous pouvez fermer le notebook, l'entraÃ®nement continuera en arriÃ¨re-plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4731888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Exportation du modÃ¨le entraÃ®nÃ© pour l'infÃ©rence...\n",
      "ğŸ“¦ Commande d'exportation:\n",
      "/home/sarsator/projets/gaia_vision/.venv/bin/python tensorflow_models/research/object_detection/exporter_main_v2.py --input_type=image_tensor --pipeline_config_path=../models/dl_model/outputs/ssd_mnv2_320/pipeline.config --trained_checkpoint_dir=../models/dl_model/outputs/ssd_mnv2_320 --output_directory=../models/dl_model/outputs/ssd_mnv2_320/exported_model\n",
      "ğŸ“‚ RÃ©pertoire de sortie: ../models/dl_model/outputs/ssd_mnv2_320/exported_model\n",
      "â³ Exportation en cours...\n",
      "âœ… EXPORTATION RÃ‰USSIE !\n",
      "ğŸ“ ModÃ¨le exportÃ© dans: ../models/dl_model/outputs/ssd_mnv2_320/exported_model\n",
      "âœ… Dossier saved_model crÃ©Ã©\n",
      "   ğŸ“„ fingerprint.pb\n",
      "   ğŸ“„ variables\n",
      "   ğŸ“„ saved_model.pb\n",
      "   ğŸ“„ assets\n",
      "âœ… Checkpoint d'exportation crÃ©Ã©\n",
      "\n",
      "ğŸ¯ Le modÃ¨le est maintenant prÃªt pour l'infÃ©rence !\n",
      "\n",
      "ğŸ“‹ RÃ©sumÃ©:\n",
      "   ğŸ¤– ModÃ¨le: SSD MobileNet V2 320x320\n",
      "   ğŸ¯ Classes: Healthy, Contaminated\n",
      "   ğŸ“ˆ EntraÃ®nement: 30,000 steps terminÃ©s\n",
      "   ğŸ’¾ ModÃ¨le exportÃ©: ../models/dl_model/outputs/ssd_mnv2_320/exported_model/saved_model\n",
      "   ğŸ“Š TensorBoard: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¯ EXPORTATION DU MODÃˆLE ENTRAÃNÃ‰ POUR INFÃ‰RENCE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ğŸš€ Exportation du modÃ¨le entraÃ®nÃ© pour l'infÃ©rence...\")\n",
    "\n",
    "# Chemins pour l'exportation\n",
    "EXPORT_DIR = f\"{OUTPUT_DIR}/exported_model\"\n",
    "CHECKPOINT_DIR = OUTPUT_DIR\n",
    "PIPELINE_CONFIG = f\"{OUTPUT_DIR}/pipeline.config\"\n",
    "\n",
    "# CrÃ©er le rÃ©pertoire d'exportation\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# Variables d'environnement\n",
    "env = os.environ.copy()\n",
    "research_path = os.path.abspath(\"tensorflow_models/research\")\n",
    "slim_path = os.path.abspath(\"tensorflow_models/research/slim\")\n",
    "env[\"PYTHONPATH\"] = f\"{research_path}:{slim_path}:{env.get('PYTHONPATH', '')}\"\n",
    "\n",
    "# Commande d'exportation\n",
    "export_cmd = [\n",
    "    sys.executable,\n",
    "    \"tensorflow_models/research/object_detection/exporter_main_v2.py\",\n",
    "    f\"--input_type=image_tensor\",\n",
    "    f\"--pipeline_config_path={PIPELINE_CONFIG}\",\n",
    "    f\"--trained_checkpoint_dir={CHECKPOINT_DIR}\",\n",
    "    f\"--output_directory={EXPORT_DIR}\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“¦ Commande d'exportation:\")\n",
    "print(\" \".join(export_cmd))\n",
    "print(f\"ğŸ“‚ RÃ©pertoire de sortie: {EXPORT_DIR}\")\n",
    "\n",
    "try:\n",
    "    print(\"â³ Exportation en cours...\")\n",
    "    result = subprocess.run(export_cmd, env=env, capture_output=True, text=True, timeout=300)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… EXPORTATION RÃ‰USSIE !\")\n",
    "        print(f\"ğŸ“ ModÃ¨le exportÃ© dans: {EXPORT_DIR}\")\n",
    "        \n",
    "        # VÃ©rifier les fichiers crÃ©Ã©s\n",
    "        if os.path.exists(f\"{EXPORT_DIR}/saved_model\"):\n",
    "            print(\"âœ… Dossier saved_model crÃ©Ã©\")\n",
    "            saved_model_files = os.listdir(f\"{EXPORT_DIR}/saved_model\")\n",
    "            for file in saved_model_files:\n",
    "                print(f\"   ğŸ“„ {file}\")\n",
    "        \n",
    "        if os.path.exists(f\"{EXPORT_DIR}/checkpoint\"):\n",
    "            print(\"âœ… Checkpoint d'exportation crÃ©Ã©\")\n",
    "            \n",
    "        print(\"\\nğŸ¯ Le modÃ¨le est maintenant prÃªt pour l'infÃ©rence !\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ ERREUR lors de l'exportation\")\n",
    "        print(f\"Code d'erreur: {result.returncode}\")\n",
    "        if result.stderr:\n",
    "            print(\"Erreurs:\")\n",
    "            print(result.stderr[-1000:])  # Afficher les 1000 derniers caractÃ¨res\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"â° Timeout - L'exportation prend plus de 5 minutes\")\n",
    "    print(\"ğŸ’¡ Essayez de relancer la cellule ou exÃ©cutez manuellement la commande\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ RÃ©sumÃ©:\")\n",
    "print(f\"   ğŸ¤– ModÃ¨le: SSD MobileNet V2 320x320\")\n",
    "print(f\"   ğŸ¯ Classes: Healthy, Contaminated\")\n",
    "print(f\"   ğŸ“ˆ EntraÃ®nement: 30,000 steps terminÃ©s\")\n",
    "print(f\"   ğŸ’¾ ModÃ¨le exportÃ©: {EXPORT_DIR}/saved_model\")\n",
    "print(f\"   ğŸ“Š TensorBoard: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "017cbe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Script d'infÃ©rence crÃ©Ã©: ../models/dl_model/outputs/ssd_mnv2_320/mushroom_inference.py\n",
      "\\nğŸ“‹ Utilisation:\n",
      "   python ../models/dl_model/outputs/ssd_mnv2_320/mushroom_inference.py \\\\\n",
      "     --model ../models/dl_model/outputs/ssd_mnv2_320/exported_model/saved_model \\\\\n",
      "     --image chemin/vers/image.jpg \\\\\n",
      "     --output resultat.jpg \\\\\n",
      "     --threshold 0.5\n",
      "\\nğŸ’¡ Exemple:\n",
      "   python ../models/dl_model/outputs/ssd_mnv2_320/mushroom_inference.py \\\\\n",
      "     --model ../models/dl_model/outputs/ssd_mnv2_320/exported_model/saved_model \\\\\n",
      "     --image ../../../images_a_traiter/test_image.jpg \\\\\n",
      "     --output detection_result.jpg\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ” SCRIPT D'INFÃ‰RENCE - TESTER LE MODÃˆLE ENTRAÃNÃ‰\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "inference_script = f'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script d'infÃ©rence pour le modÃ¨le SSD MobileNet V2 de dÃ©tection de champignons\n",
    "DÃ©tecte si un champignon est 'Healthy' ou 'Contaminated'\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "class MushroomDetector:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Initialise le dÃ©tecteur avec le modÃ¨le sauvegardÃ©\"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.class_names = {{1: 'Healthy', 2: 'Contaminated'}}\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Charge le modÃ¨le TensorFlow SavedModel\"\"\"\n",
    "        try:\n",
    "            print(f\"ğŸ“¦ Chargement du modÃ¨le: {{self.model_path}}\")\n",
    "            self.model = tf.saved_model.load(self.model_path)\n",
    "            self.infer = self.model.signatures['serving_default']\n",
    "            print(\"âœ… ModÃ¨le chargÃ© avec succÃ¨s!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur lors du chargement: {{e}}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_image(self, image_path, target_size=(320, 320)):\n",
    "        \"\"\"PrÃ©processe l'image pour l'infÃ©rence\"\"\"\n",
    "        # Lire l'image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Impossible de lire l'image: {{image_path}}\")\n",
    "        \n",
    "        # Convertir BGR vers RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Redimensionner\n",
    "        image = cv2.resize(image, target_size)\n",
    "        \n",
    "        # Normaliser (0-255 -> 0-1)\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        # Ajouter une dimension batch\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def predict(self, image_path, confidence_threshold=0.5):\n",
    "        \"\"\"Effectue la prÃ©diction sur une image\"\"\"\n",
    "        # PrÃ©processer l'image\n",
    "        input_tensor = self.preprocess_image(image_path)\n",
    "        input_tensor = tf.constant(input_tensor, dtype=tf.uint8)\n",
    "        \n",
    "        # Faire l'infÃ©rence\n",
    "        detections = self.infer(input_tensor)\n",
    "        \n",
    "        # Extraire les rÃ©sultats\n",
    "        boxes = detections['detection_boxes'][0].numpy()\n",
    "        classes = detections['detection_classes'][0].numpy().astype(int)\n",
    "        scores = detections['detection_scores'][0].numpy()\n",
    "        \n",
    "        # Filtrer par seuil de confiance\n",
    "        valid_detections = scores >= confidence_threshold\n",
    "        \n",
    "        results = []\n",
    "        for i, valid in enumerate(valid_detections):\n",
    "            if valid:\n",
    "                class_id = classes[i]\n",
    "                class_name = self.class_names.get(class_id, f\"Classe_{{class_id}}\")\n",
    "                confidence = scores[i]\n",
    "                box = boxes[i]  # [ymin, xmin, ymax, xmax] normalisÃ©\n",
    "                \n",
    "                results.append({{\n",
    "                    'class_id': class_id,\n",
    "                    'class_name': class_name,\n",
    "                    'confidence': confidence,\n",
    "                    'box': box\n",
    "                }})\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_results(self, image_path, results, output_path=None):\n",
    "        \"\"\"Visualise les rÃ©sultats sur l'image\"\"\"\n",
    "        image = cv2.imread(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        for result in results:\n",
    "            # Convertir les coordonnÃ©es normalisÃ©es en pixels\n",
    "            ymin, xmin, ymax, xmax = result['box']\n",
    "            xmin = int(xmin * width)\n",
    "            xmax = int(xmax * width)\n",
    "            ymin = int(ymin * height)\n",
    "            ymax = int(ymax * height)\n",
    "            \n",
    "            # Couleur selon la classe\n",
    "            color = (0, 255, 0) if result['class_name'] == 'Healthy' else (0, 0, 255)\n",
    "            \n",
    "            # Dessiner la boÃ®te\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "            \n",
    "            # Ajouter le texte\n",
    "            label = f\"{{result['class_name']}}: {{result['confidence']:.2f}}\"\n",
    "            cv2.putText(image, label, (xmin, ymin-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        if output_path:\n",
    "            cv2.imwrite(output_path, image)\n",
    "            print(f\"ğŸ’¾ Image sauvÃ©e: {{output_path}}\")\n",
    "        else:\n",
    "            cv2.imshow('DÃ©tection', image)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        return image\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='DÃ©tection de champignons')\n",
    "    parser.add_argument('--model', required=True, help='Chemin vers le modÃ¨le SavedModel')\n",
    "    parser.add_argument('--image', required=True, help='Chemin vers l\\\\'image Ã  analyser')\n",
    "    parser.add_argument('--output', help='Chemin de sortie pour l\\\\'image annotÃ©e')\n",
    "    parser.add_argument('--threshold', type=float, default=0.5, help='Seuil de confiance')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialiser le dÃ©tecteur\n",
    "    detector = MushroomDetector(args.model)\n",
    "    \n",
    "    # Faire la prÃ©diction\n",
    "    print(f\"ğŸ” Analyse de l'image: {{args.image}}\")\n",
    "    results = detector.predict(args.image, args.threshold)\n",
    "    \n",
    "    # Afficher les rÃ©sultats\n",
    "    if results:\n",
    "        print(f\"\\\\nğŸ¯ {{len(results)}} dÃ©tection(s) trouvÃ©e(s):\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {{i}}. {{result['class_name']}} ({{result['confidence']:.1%}})\")\n",
    "    else:\n",
    "        print(\"âŒ Aucune dÃ©tection au-dessus du seuil de confiance\")\n",
    "    \n",
    "    # Visualiser si demandÃ©\n",
    "    if args.output or len(results) > 0:\n",
    "        detector.visualize_results(args.image, results, args.output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Sauvegarder le script d'infÃ©rence\n",
    "inference_script_path = f\"{OUTPUT_DIR}/mushroom_inference.py\"\n",
    "with open(inference_script_path, 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "# Rendre le script exÃ©cutable\n",
    "os.chmod(inference_script_path, 0o755)\n",
    "\n",
    "print(f\"âœ… Script d'infÃ©rence crÃ©Ã©: {inference_script_path}\")\n",
    "print(f\"\\\\nğŸ“‹ Utilisation:\")\n",
    "print(f\"   python {inference_script_path} \\\\\\\\\")\n",
    "print(f\"     --model {EXPORT_DIR}/saved_model \\\\\\\\\")\n",
    "print(f\"     --image chemin/vers/image.jpg \\\\\\\\\")\n",
    "print(f\"     --output resultat.jpg \\\\\\\\\")\n",
    "print(f\"     --threshold 0.5\")\n",
    "\n",
    "print(f\"\\\\nğŸ’¡ Exemple:\")\n",
    "print(f\"   python {inference_script_path} \\\\\\\\\")\n",
    "print(f\"     --model {EXPORT_DIR}/saved_model \\\\\\\\\")\n",
    "print(f\"     --image ../../../images_a_traiter/test_image.jpg \\\\\\\\\")\n",
    "print(f\"     --output detection_result.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa6028",
   "metadata": {},
   "source": [
    "# ğŸ‰ FÃ‰LICITATIONS ! ENTRAÃNEMENT TERMINÃ‰\n",
    "\n",
    "## âœ… RÃ©sumÃ© de l'entraÃ®nement rÃ©ussi :\n",
    "\n",
    "- **ModÃ¨le** : SSD MobileNet V2 320x320\n",
    "- **Steps** : 30,000 steps complÃ©tÃ©s\n",
    "- **Loss finale** : ~0.215 (excellent !)\n",
    "- **GPU utilisÃ©** : RTX 4080 avec succÃ¨s\n",
    "- **DurÃ©e** : ~52 minutes d'entraÃ®nement\n",
    "\n",
    "## ğŸš€ Prochaines Ã©tapes :\n",
    "\n",
    "### 1ï¸âƒ£ **Exporter le modÃ¨le**\n",
    "ExÃ©cutez la cellule prÃ©cÃ©dente pour exporter le modÃ¨le au format SavedModel\n",
    "\n",
    "### 2ï¸âƒ£ **Tester sur de nouvelles images**\n",
    "```bash\n",
    "# Exemple d'utilisation du script d'infÃ©rence\n",
    "python mushroom_inference.py \\\n",
    "  --model exported_model/saved_model \\\n",
    "  --image ../../../../images_a_traiter/test_image.jpg \\\n",
    "  --output resultat_detection.jpg \\\n",
    "  --threshold 0.5\n",
    "```\n",
    "\n",
    "### 3ï¸âƒ£ **IntÃ©grer dans votre application**\n",
    "- Le modÃ¨le exportÃ© peut Ãªtre utilisÃ© avec TensorFlow Serving\n",
    "- Compatible avec TensorFlow Lite pour mobile\n",
    "- Peut Ãªtre intÃ©grÃ© dans votre API Flask/FastAPI existante\n",
    "\n",
    "## ğŸ“Š Analyser les performances :\n",
    "\n",
    "- **TensorBoard** : http://localhost:6006\n",
    "- **Checkpoints** : SauvÃ©s automatiquement toutes les 1000 steps\n",
    "- **MÃ©triques** : Loss de classification et localisation convergent bien\n",
    "\n",
    "## ğŸ¯ Classes dÃ©tectÃ©es :\n",
    "\n",
    "1. **Healthy** : Champignons sains\n",
    "2. **Contaminated** : Champignons contaminÃ©s\n",
    "\n",
    "## ğŸ’¡ Optimisations possibles :\n",
    "\n",
    "- **Augmentation des donnÃ©es** : Rotation, flip, contraste\n",
    "- **HyperparamÃ¨tres** : Ajuster learning rate, batch size\n",
    "- **Architecture** : Tester EfficientDet, YOLOv8\n",
    "- **Post-processing** : Ajuster NMS threshold\n",
    "\n",
    "## ğŸ”§ Troubleshooting :\n",
    "\n",
    "Si vous rencontrez des problÃ¨mes :\n",
    "1. VÃ©rifiez que TensorBoard affiche bien les mÃ©triques\n",
    "2. Consultez les logs d'entraÃ®nement pour dÃ©tecter des erreurs\n",
    "3. Testez l'infÃ©rence sur quelques images de validation\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸŠ Votre modÃ¨le de dÃ©tection de champignons est maintenant prÃªt Ã  l'emploi !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aad330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Test du modÃ¨le entraÃ®nÃ© sur les donnÃ©es de test...\n",
      "âœ… DonnÃ©es de test trouvÃ©es: ../data/DL_data/test\n",
      "âœ… Annotations: ../data/DL_data/test/_annotations.csv\n",
      "âœ… ModÃ¨le: ../models/dl_model/outputs/ssd_mnv2_320/exported_model/saved_model\n",
      "ğŸ“¦ Chargement du modÃ¨le...\n",
      "âœ… ModÃ¨le chargÃ© avec succÃ¨s!\n",
      "ğŸ“Š Lecture des annotations de test...\n",
      "ğŸ“ˆ Dataset de test: 532 annotations, 394 images uniques\n",
      "ğŸ“‹ Classes dans le test: {'Healthy': 305, 'Contaminated': 227}\n",
      "\\nğŸš€ DÃ©marrage de l'Ã©valuation...\n",
      "ğŸ” Ã‰valuation sur 394 images de test...\n",
      "   Seuil de confiance: 0.5\n",
      "   Seuil IoU: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 02:09:14.678539: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸  Temps d'infÃ©rence moyen: 0.016s par image\n",
      "\\nğŸ“Š RÃ©sultats de l'Ã©valuation:\n",
      "   Total d'Ã©valuations: 532\n",
      "   True Positives (TP): 510\n",
      "   False Positives (FP): 1\n",
      "   False Negatives (FN): 21\n",
      "\\nğŸ¯ MÃ©triques de performance:\n",
      "   PrÃ©cision: 0.998 (99.8%)\n",
      "   Rappel: 0.960 (96.0%)\n",
      "   F1-Score: 0.979\n",
      "\\nğŸ“ˆ Performance par classe:\n",
      "   Healthy:\n",
      "     PrÃ©cision: 1.000 (100.0%)\n",
      "     Rappel: 0.957 (95.7%)\n",
      "     F1-Score: 0.978\n",
      "   Contaminated:\n",
      "     PrÃ©cision: 0.995 (99.5%)\n",
      "     Rappel: 0.965 (96.5%)\n",
      "     F1-Score: 0.980\n",
      "\\nğŸ’ª Confiance moyenne des prÃ©dictions: 0.817 (81.7%)\n",
      "\\nâœ… Ã‰valuation terminÃ©e avec succÃ¨s!\n",
      "ğŸ“Š DataFrame 'results_df' disponible pour analyse dÃ©taillÃ©e\n",
      "\\nğŸ’¾ Variables crÃ©Ã©es:\n",
      "   - test_df: DataFrame des annotations de test\n",
      "   - results_df: DataFrame des rÃ©sultats d'Ã©valuation\n",
      "   - tester: Instance du testeur pour analyses supplÃ©mentaires\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ§ª TEST DU MODÃˆLE SUR LES DONNÃ‰ES DE TEST\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "print(\"ğŸ§ª Test du modÃ¨le entraÃ®nÃ© sur les donnÃ©es de test...\")\n",
    "\n",
    "# Chemins vers les donnÃ©es de test\n",
    "TEST_DIR = \"../data/DL_data/test\"\n",
    "TEST_ANN = f\"{TEST_DIR}/_annotations.csv\"\n",
    "SAVED_MODEL_PATH = f\"{EXPORT_DIR}/saved_model\"\n",
    "\n",
    "# VÃ©rification des fichiers\n",
    "if not os.path.exists(TEST_ANN):\n",
    "    print(f\"âŒ Fichier d'annotations de test introuvable: {TEST_ANN}\")\n",
    "    raise FileNotFoundError(f\"Annotations de test manquantes: {TEST_ANN}\")\n",
    "\n",
    "if not os.path.exists(SAVED_MODEL_PATH):\n",
    "    print(f\"âŒ ModÃ¨le exportÃ© introuvable: {SAVED_MODEL_PATH}\")\n",
    "    print(\"ğŸ’¡ ExÃ©cutez d'abord la cellule d'exportation du modÃ¨le\")\n",
    "    raise FileNotFoundError(f\"ModÃ¨le exportÃ© manquant: {SAVED_MODEL_PATH}\")\n",
    "\n",
    "print(f\"âœ… DonnÃ©es de test trouvÃ©es: {TEST_DIR}\")\n",
    "print(f\"âœ… Annotations: {TEST_ANN}\")\n",
    "print(f\"âœ… ModÃ¨le: {SAVED_MODEL_PATH}\")\n",
    "\n",
    "# Chargement du modÃ¨le\n",
    "print(\"ğŸ“¦ Chargement du modÃ¨le...\")\n",
    "model = tf.saved_model.load(SAVED_MODEL_PATH)\n",
    "infer_fn = model.signatures['serving_default']\n",
    "print(\"âœ… ModÃ¨le chargÃ© avec succÃ¨s!\")\n",
    "\n",
    "# Lecture des annotations de test\n",
    "print(\"ğŸ“Š Lecture des annotations de test...\")\n",
    "test_df = pd.read_csv(TEST_ANN)\n",
    "print(f\"ğŸ“ˆ Dataset de test: {len(test_df)} annotations, {test_df['filename'].nunique()} images uniques\")\n",
    "print(f\"ğŸ“‹ Classes dans le test: {test_df['class'].value_counts().to_dict()}\")\n",
    "\n",
    "# Classe de dÃ©tection\n",
    "class MushroomTester:\n",
    "    def __init__(self, model_fn, class_names={1: 'Healthy', 2: 'Contaminated'}):\n",
    "        self.model_fn = model_fn\n",
    "        self.class_names = class_names\n",
    "        self.results = []\n",
    "    \n",
    "    def preprocess_image(self, image_path, target_size=(320, 320)):\n",
    "        \"\"\"PrÃ©processe l'image pour l'infÃ©rence\"\"\"\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Impossible de lire l'image: {image_path}\")\n",
    "        \n",
    "        # Convertir BGR vers RGB et redimensionner\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, target_size)\n",
    "        \n",
    "        # Convertir en uint8 et ajouter dimension batch\n",
    "        image = image.astype(np.uint8)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def predict_image(self, image_path, confidence_threshold=0.5):\n",
    "        \"\"\"PrÃ©dit sur une seule image\"\"\"\n",
    "        try:\n",
    "            # PrÃ©processer\n",
    "            input_tensor = self.preprocess_image(image_path)\n",
    "            input_tensor = tf.constant(input_tensor, dtype=tf.uint8)\n",
    "            \n",
    "            # InfÃ©rence\n",
    "            start_time = time.time()\n",
    "            detections = self.model_fn(input_tensor)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Extraire rÃ©sultats\n",
    "            boxes = detections['detection_boxes'][0].numpy()\n",
    "            classes = detections['detection_classes'][0].numpy().astype(int)\n",
    "            scores = detections['detection_scores'][0].numpy()\n",
    "            \n",
    "            # Filtrer par confiance\n",
    "            valid_detections = scores >= confidence_threshold\n",
    "            \n",
    "            predictions = []\n",
    "            for i, valid in enumerate(valid_detections):\n",
    "                if valid:\n",
    "                    predictions.append({\n",
    "                        'class_id': classes[i],\n",
    "                        'class_name': self.class_names.get(classes[i], f\"Class_{classes[i]}\"),\n",
    "                        'confidence': scores[i],\n",
    "                        'box': boxes[i]  # [ymin, xmin, ymax, xmax]\n",
    "                    })\n",
    "            \n",
    "            return predictions, inference_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur lors de la prÃ©diction pour {image_path}: {e}\")\n",
    "            return [], 0.0\n",
    "    \n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"Calcule l'IoU entre deux boÃ®tes [ymin, xmin, ymax, xmax]\"\"\"\n",
    "        # CoordonnÃ©es d'intersection\n",
    "        y1 = max(box1[0], box2[0])\n",
    "        x1 = max(box1[1], box2[1])\n",
    "        y2 = min(box1[2], box2[2])\n",
    "        x2 = min(box1[3], box2[3])\n",
    "        \n",
    "        # Aire d'intersection\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = (x2 - x1) * (y2 - y1)\n",
    "        \n",
    "        # Aires des boÃ®tes\n",
    "        area1 = (box1[3] - box1[1]) * (box1[2] - box1[0])\n",
    "        area2 = (box2[3] - box2[1]) * (box2[2] - box2[0])\n",
    "        \n",
    "        # IoU\n",
    "        union = area1 + area2 - intersection\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def evaluate_on_test_set(self, test_df, test_dir, confidence_threshold=0.5, iou_threshold=0.5):\n",
    "        \"\"\"Ã‰value le modÃ¨le sur le dataset de test complet\"\"\"\n",
    "        print(f\"ğŸ” Ã‰valuation sur {test_df['filename'].nunique()} images de test...\")\n",
    "        print(f\"   Seuil de confiance: {confidence_threshold}\")\n",
    "        print(f\"   Seuil IoU: {iou_threshold}\")\n",
    "        \n",
    "        results = []\n",
    "        total_inference_time = 0\n",
    "        \n",
    "        # Grouper par image\n",
    "        for filename, group in test_df.groupby('filename'):\n",
    "            image_path = os.path.join(test_dir, filename)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"âš ï¸  Image manquante: {filename}\")\n",
    "                continue\n",
    "            \n",
    "            # PrÃ©dictions du modÃ¨le\n",
    "            predictions, inf_time = self.predict_image(image_path, confidence_threshold)\n",
    "            total_inference_time += inf_time\n",
    "            \n",
    "            # VÃ©ritÃ©s terrain (ground truth)\n",
    "            gt_boxes = []\n",
    "            for _, row in group.iterrows():\n",
    "                # Normaliser les coordonnÃ©es ground truth\n",
    "                gt_box = [\n",
    "                    row['ymin'] / row['height'],  # ymin\n",
    "                    row['xmin'] / row['width'],   # xmin\n",
    "                    row['ymax'] / row['height'],  # ymax\n",
    "                    row['xmax'] / row['width']    # xmax\n",
    "                ]\n",
    "                gt_boxes.append({\n",
    "                    'box': gt_box,\n",
    "                    'class_name': row['class'],\n",
    "                    'class_id': 1 if row['class'] == 'Healthy' else 2\n",
    "                })\n",
    "            \n",
    "            # Associer prÃ©dictions et ground truth\n",
    "            for gt in gt_boxes:\n",
    "                best_iou = 0\n",
    "                best_pred = None\n",
    "                \n",
    "                for pred in predictions:\n",
    "                    iou = self.calculate_iou(gt['box'], pred['box'])\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_pred = pred\n",
    "                \n",
    "                # DÃ©terminer si c'est un TP, FP, ou FN\n",
    "                if best_iou >= iou_threshold and best_pred:\n",
    "                    # True Positive si mÃªme classe\n",
    "                    is_correct = (gt['class_id'] == best_pred['class_id'])\n",
    "                    results.append({\n",
    "                        'filename': filename,\n",
    "                        'gt_class': gt['class_name'],\n",
    "                        'pred_class': best_pred['class_name'],\n",
    "                        'confidence': best_pred['confidence'],\n",
    "                        'iou': best_iou,\n",
    "                        'type': 'TP' if is_correct else 'FP',\n",
    "                        'inference_time': inf_time\n",
    "                    })\n",
    "                else:\n",
    "                    # False Negative\n",
    "                    results.append({\n",
    "                        'filename': filename,\n",
    "                        'gt_class': gt['class_name'],\n",
    "                        'pred_class': 'None',\n",
    "                        'confidence': 0.0,\n",
    "                        'iou': 0.0,\n",
    "                        'type': 'FN',\n",
    "                        'inference_time': inf_time\n",
    "                    })\n",
    "            \n",
    "            # False Positives (prÃ©dictions sans correspondance GT)\n",
    "            for pred in predictions:\n",
    "                has_match = False\n",
    "                for gt in gt_boxes:\n",
    "                    if self.calculate_iou(gt['box'], pred['box']) >= iou_threshold:\n",
    "                        has_match = True\n",
    "                        break\n",
    "                \n",
    "                if not has_match:\n",
    "                    results.append({\n",
    "                        'filename': filename,\n",
    "                        'gt_class': 'None',\n",
    "                        'pred_class': pred['class_name'],\n",
    "                        'confidence': pred['confidence'],\n",
    "                        'iou': 0.0,\n",
    "                        'type': 'FP',\n",
    "                        'inference_time': inf_time\n",
    "                    })\n",
    "        \n",
    "        self.results = results\n",
    "        \n",
    "        # Statistiques\n",
    "        avg_inference_time = total_inference_time / len(test_df['filename'].unique())\n",
    "        print(f\"â±ï¸  Temps d'infÃ©rence moyen: {avg_inference_time:.3f}s par image\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialisation du testeur\n",
    "tester = MushroomTester(infer_fn)\n",
    "\n",
    "# ExÃ©cution des tests\n",
    "print(\"\\\\nğŸš€ DÃ©marrage de l'Ã©valuation...\")\n",
    "test_results = tester.evaluate_on_test_set(test_df, TEST_DIR, confidence_threshold=0.5, iou_threshold=0.5)\n",
    "\n",
    "# Conversion en DataFrame pour l'analyse\n",
    "results_df = pd.DataFrame(test_results)\n",
    "\n",
    "print(f\"\\\\nğŸ“Š RÃ©sultats de l'Ã©valuation:\")\n",
    "print(f\"   Total d'Ã©valuations: {len(results_df)}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    type_counts = results_df['type'].value_counts()\n",
    "    print(f\"   True Positives (TP): {type_counts.get('TP', 0)}\")\n",
    "    print(f\"   False Positives (FP): {type_counts.get('FP', 0)}\")\n",
    "    print(f\"   False Negatives (FN): {type_counts.get('FN', 0)}\")\n",
    "    \n",
    "    # Calcul des mÃ©triques\n",
    "    tp = type_counts.get('TP', 0)\n",
    "    fp = type_counts.get('FP', 0)\n",
    "    fn = type_counts.get('FN', 0)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\\\nğŸ¯ MÃ©triques de performance:\")\n",
    "    print(f\"   PrÃ©cision: {precision:.3f} ({precision*100:.1f}%)\")\n",
    "    print(f\"   Rappel: {recall:.3f} ({recall*100:.1f}%)\")\n",
    "    print(f\"   F1-Score: {f1_score:.3f}\")\n",
    "    \n",
    "    # MÃ©triques par classe\n",
    "    print(f\"\\\\nğŸ“ˆ Performance par classe:\")\n",
    "    for class_name in ['Healthy', 'Contaminated']:\n",
    "        class_tp = len(results_df[(results_df['gt_class'] == class_name) & (results_df['type'] == 'TP')])\n",
    "        class_fp = len(results_df[(results_df['pred_class'] == class_name) & (results_df['type'] == 'FP')])\n",
    "        class_fn = len(results_df[(results_df['gt_class'] == class_name) & (results_df['type'] == 'FN')])\n",
    "        \n",
    "        class_precision = class_tp / (class_tp + class_fp) if (class_tp + class_fp) > 0 else 0\n",
    "        class_recall = class_tp / (class_tp + class_fn) if (class_tp + class_fn) > 0 else 0\n",
    "        class_f1 = 2 * (class_precision * class_recall) / (class_precision + class_recall) if (class_precision + class_recall) > 0 else 0\n",
    "        \n",
    "        print(f\"   {class_name}:\")\n",
    "        print(f\"     PrÃ©cision: {class_precision:.3f} ({class_precision*100:.1f}%)\")\n",
    "        print(f\"     Rappel: {class_recall:.3f} ({class_recall*100:.1f}%)\")\n",
    "        print(f\"     F1-Score: {class_f1:.3f}\")\n",
    "    \n",
    "    # Confiance moyenne\n",
    "    if len(results_df[results_df['confidence'] > 0]) > 0:\n",
    "        avg_confidence = results_df[results_df['confidence'] > 0]['confidence'].mean()\n",
    "        print(f\"\\\\nğŸ’ª Confiance moyenne des prÃ©dictions: {avg_confidence:.3f} ({avg_confidence*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\\\nâœ… Ã‰valuation terminÃ©e avec succÃ¨s!\")\n",
    "    print(f\"ğŸ“Š DataFrame 'results_df' disponible pour analyse dÃ©taillÃ©e\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Aucun rÃ©sultat d'Ã©valuation trouvÃ©\")\n",
    "\n",
    "print(f\"\\\\nğŸ’¾ Variables crÃ©Ã©es:\")\n",
    "print(f\"   - test_df: DataFrame des annotations de test\")\n",
    "print(f\"   - results_df: DataFrame des rÃ©sultats d'Ã©valuation\")\n",
    "print(f\"   - tester: Instance du testeur pour analyses supplÃ©mentaires\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2e8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ CrÃ©ation des visualisations des rÃ©sultats...\n",
      "\\nğŸ–¼ï¸  Affichage d'exemples de prÃ©dictions...\n",
      "\\nğŸ“Š Tableau rÃ©capitulatif des performances:\n",
      "============================================================\n",
      "ğŸ¯ PERFORMANCE GLOBALE:\n",
      "   PrÃ©cision: 0.998 (99.8%)\n",
      "   Rappel: 0.960 (96.0%)\n",
      "   F1-Score: 0.979\n",
      "   DÃ©tections correctes: 510/531\n",
      "\\nğŸ“ˆ DÃ‰TAILS:\n",
      "   True Positives: 510\n",
      "   False Positives: 1\n",
      "   False Negatives: 21\n",
      "   Temps moyen d'infÃ©rence: 0.015s par image\n",
      "============================================================\n",
      "\\nâœ… Visualisation terminÃ©e!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_533743/2755555373.py:84: UserWarning: Glyph 129514 (\\N{TEST TUBE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_533743/2755555373.py:85: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/tmp/ipykernel_533743/2755555373.py:168: UserWarning: Glyph 128444 (\\N{FRAME WITH PICTURE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_533743/2755555373.py:169: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau sâ€™est bloquÃ© lors de lâ€™exÃ©cution du code dans une cellule active ou une cellule prÃ©cÃ©dente. \n",
      "\u001b[1;31mVeuillez vÃ©rifier le code dans la ou les cellules pour identifier une cause possible de lâ€™Ã©chec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus dâ€™informations. \n",
      "\u001b[1;31mPour plus dâ€™informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“ˆ VISUALISATION DES RÃ‰SULTATS ET EXEMPLES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "import random\n",
    "\n",
    "print(\"ğŸ“ˆ CrÃ©ation des visualisations des rÃ©sultats...\")\n",
    "\n",
    "# Configuration de matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# VÃ©rifier si nous avons des rÃ©sultats\n",
    "if 'results_df' in locals() and len(results_df) > 0:\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # ğŸ“Š GRAPHIQUES DE PERFORMANCE\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ğŸ§ª RÃ©sultats des Tests du ModÃ¨le SSD MobileNet V2', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Distribution des types de rÃ©sultats\n",
    "    ax1 = axes[0, 0]\n",
    "    type_counts = results_df['type'].value_counts()\n",
    "    colors = ['#2ecc71', '#e74c3c', '#f39c12']  # Vert, Rouge, Orange\n",
    "    wedges, texts, autotexts = ax1.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%', \n",
    "                                       colors=colors, startangle=90)\n",
    "    ax1.set_title('Distribution des RÃ©sultats\\\\n(TP/FP/FN)', fontweight='bold')\n",
    "    \n",
    "    # 2. Distribution des confiances\n",
    "    ax2 = axes[0, 1]\n",
    "    confident_preds = results_df[results_df['confidence'] > 0]['confidence']\n",
    "    if len(confident_preds) > 0:\n",
    "        ax2.hist(confident_preds, bins=20, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "        ax2.axvline(confident_preds.mean(), color='red', linestyle='--', \n",
    "                   label=f'Moyenne: {confident_preds.mean():.3f}')\n",
    "        ax2.set_xlabel('Confiance')\n",
    "        ax2.set_ylabel('Nombre de prÃ©dictions')\n",
    "        ax2.set_title('Distribution des Confiances', fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Performance par classe\n",
    "    ax3 = axes[1, 0]\n",
    "    class_metrics = {}\n",
    "    for class_name in ['Healthy', 'Contaminated']:\n",
    "        tp = len(results_df[(results_df['gt_class'] == class_name) & (results_df['type'] == 'TP')])\n",
    "        fp = len(results_df[(results_df['pred_class'] == class_name) & (results_df['type'] == 'FP')])\n",
    "        fn = len(results_df[(results_df['gt_class'] == class_name) & (results_df['type'] == 'FN')])\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        class_metrics[class_name] = {'PrÃ©cision': precision, 'Rappel': recall, 'F1-Score': f1}\n",
    "    \n",
    "    # Graphique en barres pour les mÃ©triques par classe\n",
    "    metrics_df = pd.DataFrame(class_metrics).T\n",
    "    metrics_df.plot(kind='bar', ax=ax3, width=0.8)\n",
    "    ax3.set_title('MÃ©triques par Classe', fontweight='bold')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.set_xlabel('Classe')\n",
    "    ax3.legend(loc='upper right')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    # 4. Distribution des IoU pour les TP\n",
    "    ax4 = axes[1, 1]\n",
    "    tp_ious = results_df[results_df['type'] == 'TP']['iou']\n",
    "    if len(tp_ious) > 0:\n",
    "        ax4.hist(tp_ious, bins=15, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "        ax4.axvline(tp_ious.mean(), color='red', linestyle='--', \n",
    "                   label=f'Moyenne: {tp_ious.mean():.3f}')\n",
    "        ax4.set_xlabel('IoU')\n",
    "        ax4.set_ylabel('Nombre de TP')\n",
    "        ax4.set_title('Distribution des IoU (True Positives)', fontweight='bold')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # ğŸ–¼ï¸ EXEMPLES DE PRÃ‰DICTIONS\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    print(\"\\\\nğŸ–¼ï¸  Affichage d'exemples de prÃ©dictions...\")\n",
    "    \n",
    "    def show_prediction_examples(results_df, test_dir, num_examples=6):\n",
    "        \"\"\"Affiche des exemples de prÃ©dictions avec les images\"\"\"\n",
    "        \n",
    "        # SÃ©lectionner des exemples variÃ©s\n",
    "        examples = []\n",
    "        \n",
    "        # Quelques TP de chaque classe\n",
    "        tp_healthy = results_df[(results_df['type'] == 'TP') & (results_df['gt_class'] == 'Healthy')]\n",
    "        tp_contaminated = results_df[(results_df['type'] == 'TP') & (results_df['gt_class'] == 'Contaminated')]\n",
    "        \n",
    "        # Quelques erreurs\n",
    "        fp_examples = results_df[results_df['type'] == 'FP']\n",
    "        fn_examples = results_df[results_df['type'] == 'FN']\n",
    "        \n",
    "        # SÃ©lection Ã©quilibrÃ©e\n",
    "        if len(tp_healthy) > 0:\n",
    "            examples.append(tp_healthy.iloc[0])\n",
    "        if len(tp_contaminated) > 0:\n",
    "            examples.append(tp_contaminated.iloc[0])\n",
    "        if len(fp_examples) > 0:\n",
    "            examples.append(fp_examples.iloc[0])\n",
    "        if len(fn_examples) > 0:\n",
    "            examples.append(fn_examples.iloc[0])\n",
    "        \n",
    "        # ComplÃ©ter avec des exemples alÃ©atoires\n",
    "        remaining = results_df.sample(min(num_examples - len(examples), len(results_df)))\n",
    "        for _, row in remaining.iterrows():\n",
    "            if len(examples) < num_examples and row.name not in [ex.name for ex in examples]:\n",
    "                examples.append(row)\n",
    "        \n",
    "        # CrÃ©er la figure\n",
    "        n_examples = min(len(examples), num_examples)\n",
    "        cols = 3\n",
    "        rows = (n_examples + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, example in enumerate(examples[:num_examples]):\n",
    "            row, col = i // cols, i % cols\n",
    "            ax = axes[row, col] if rows > 1 else axes[col]\n",
    "            \n",
    "            # Charger et afficher l'image\n",
    "            image_path = os.path.join(test_dir, example['filename'])\n",
    "            if os.path.exists(image_path):\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                ax.imshow(image)\n",
    "                \n",
    "                # Titre avec les informations\n",
    "                title = f\"{example['type']}: {example['filename'][:20]}...\"\n",
    "                if example['gt_class'] != 'None':\n",
    "                    title += f\"\\\\nGT: {example['gt_class']}\"\n",
    "                if example['pred_class'] != 'None':\n",
    "                    title += f\" | Pred: {example['pred_class']} ({example['confidence']:.2f})\"\n",
    "                if example['iou'] > 0:\n",
    "                    title += f\"\\\\nIoU: {example['iou']:.3f}\"\n",
    "                \n",
    "                # Couleur du titre selon le type\n",
    "                title_color = {'TP': 'green', 'FP': 'red', 'FN': 'orange'}.get(example['type'], 'black')\n",
    "                ax.set_title(title, fontsize=10, color=title_color, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f\"Image\\\\n{example['filename']}\\\\nnon trouvÃ©e\", \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "            \n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Masquer les axes vides\n",
    "        for i in range(n_examples, rows * cols):\n",
    "            row, col = i // cols, i % cols\n",
    "            ax = axes[row, col] if rows > 1 else axes[col]\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.suptitle('ğŸ–¼ï¸ Exemples de PrÃ©dictions du ModÃ¨le', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Afficher les exemples\n",
    "    show_prediction_examples(results_df, TEST_DIR, num_examples=6)\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # ğŸ“Š TABLEAU RÃ‰CAPITULATIF\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    print(\"\\\\nğŸ“Š Tableau rÃ©capitulatif des performances:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculs globaux\n",
    "    tp_total = len(results_df[results_df['type'] == 'TP'])\n",
    "    fp_total = len(results_df[results_df['type'] == 'FP'])\n",
    "    fn_total = len(results_df[results_df['type'] == 'FN'])\n",
    "    \n",
    "    precision_global = tp_total / (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0\n",
    "    recall_global = tp_total / (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0\n",
    "    f1_global = 2 * (precision_global * recall_global) / (precision_global + recall_global) if (precision_global + recall_global) > 0 else 0\n",
    "    \n",
    "    print(f\"ğŸ¯ PERFORMANCE GLOBALE:\")\n",
    "    print(f\"   PrÃ©cision: {precision_global:.3f} ({precision_global*100:.1f}%)\")\n",
    "    print(f\"   Rappel: {recall_global:.3f} ({recall_global*100:.1f}%)\")\n",
    "    print(f\"   F1-Score: {f1_global:.3f}\")\n",
    "    print(f\"   DÃ©tections correctes: {tp_total}/{tp_total + fn_total}\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ“ˆ DÃ‰TAILS:\")\n",
    "    print(f\"   True Positives: {tp_total}\")\n",
    "    print(f\"   False Positives: {fp_total}\")\n",
    "    print(f\"   False Negatives: {fn_total}\")\n",
    "    \n",
    "    # Temps d'infÃ©rence\n",
    "    if 'inference_time' in results_df.columns:\n",
    "        avg_time = results_df['inference_time'].mean()\n",
    "        print(f\"   Temps moyen d'infÃ©rence: {avg_time:.3f}s par image\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Aucun rÃ©sultat Ã  visualiser. ExÃ©cutez d'abord la cellule de test prÃ©cÃ©dente.\")\n",
    "\n",
    "print(\"\\\\nâœ… Visualisation terminÃ©e!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
