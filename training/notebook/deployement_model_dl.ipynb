{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a587b5c",
   "metadata": {},
   "source": [
    "# 🚀 Déploiement du Modèle SSD MobileNet V2\n",
    "\n",
    "Ce notebook automatise le déploiement du modèle SSD MobileNet V2 entraîné vers le système de versioning de l'API.\n",
    "\n",
    "## 📋 Processus de déploiement\n",
    "\n",
    "1. **Analyse du modèle** : Vérification du modèle SavedModel\n",
    "2. **Versioning automatique** : Incrémentation intelligente de la version\n",
    "3. **Copie et métadonnées** : Déploiement avec informations complètes\n",
    "4. **Validation** : Test du modèle déployé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07caf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 13:25:11.048155: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-16 13:25:11.072481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-16 13:25:11.072515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-16 13:25:11.073274: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-16 13:25:11.078016: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-16 13:25:11.819337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Configuration du déploiement\n",
      "📁 Modèle source: /home/sarsator/projets/gaia_vision/training/models/dl_model/outputs/ssd_mnv2_320/exported_model/saved_model\n",
      "📁 Destination API: /home/sarsator/projets/gaia_vision/api/models/dl_model/versions\n",
      "📁 Notebook source: /home/sarsator/projets/gaia_vision/training/notebook/dl_finetuning.ipynb\n",
      "✅ Tous les chemins sont valides!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Configuration des chemins\n",
    "TRAINING_MODEL_PATH = \"/home/sarsator/projets/gaia_vision/training/models/dl_model/outputs/ssd_mnv2_320/exported_model/saved_model\"\n",
    "API_VERSIONS_PATH = \"/home/sarsator/projets/gaia_vision/api/models/dl_model/versions\"\n",
    "TRAINING_NOTEBOOK_PATH = \"/home/sarsator/projets/gaia_vision/training/notebook/dl_finetuning.ipynb\"\n",
    "\n",
    "print(\"🔧 Configuration du déploiement\")\n",
    "print(f\"📁 Modèle source: {TRAINING_MODEL_PATH}\")\n",
    "print(f\"📁 Destination API: {API_VERSIONS_PATH}\")\n",
    "print(f\"📁 Notebook source: {TRAINING_NOTEBOOK_PATH}\")\n",
    "\n",
    "# Vérification des chemins\n",
    "assert os.path.exists(TRAINING_MODEL_PATH), f\"❌ Modèle source non trouvé: {TRAINING_MODEL_PATH}\"\n",
    "assert os.path.exists(API_VERSIONS_PATH), f\"❌ Dossier de versions non trouvé: {API_VERSIONS_PATH}\"\n",
    "assert os.path.exists(TRAINING_NOTEBOOK_PATH), f\"❌ Notebook source non trouvé: {TRAINING_NOTEBOOK_PATH}\"\n",
    "\n",
    "print(\"✅ Tous les chemins sont valides!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5205d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Versions existantes trouvées:\n",
      "   └── v1.0_20250711_143245\n",
      "   └── v1.1_20250711_143319\n",
      "   └── v1.2_20250711_143350\n",
      "🔍 Dernière version: v1.2\n",
      "🆕 Nouvelle version: v1.3_20250716_132518\n"
     ]
    }
   ],
   "source": [
    "def get_next_version():\n",
    "    \"\"\"Analyse les versions existantes et retourne la prochaine version à utiliser\"\"\"\n",
    "    \n",
    "    if not os.path.exists(API_VERSIONS_PATH):\n",
    "        return \"1.0\", \"20250716_000000\"\n",
    "    \n",
    "    # Lister toutes les versions existantes\n",
    "    versions = []\n",
    "    for folder in os.listdir(API_VERSIONS_PATH):\n",
    "        if os.path.isdir(os.path.join(API_VERSIONS_PATH, folder)):\n",
    "            # Format attendu: v1.2_20250711_143350\n",
    "            match = re.match(r'v(\\d+)\\.(\\d+)_(\\d{8}_\\d{6})', folder)\n",
    "            if match:\n",
    "                major, minor, timestamp = match.groups()\n",
    "                versions.append((int(major), int(minor), timestamp, folder))\n",
    "    \n",
    "    if not versions:\n",
    "        return \"1.0\", \"20250716_000000\"\n",
    "    \n",
    "    # Trier par version (major, minor)\n",
    "    versions.sort(key=lambda x: (x[0], x[1]))\n",
    "    latest_major, latest_minor, _, latest_folder = versions[-1]\n",
    "    \n",
    "    print(f\"📋 Versions existantes trouvées:\")\n",
    "    for major, minor, timestamp, folder in versions:\n",
    "        print(f\"   └── v{major}.{minor}_{timestamp}\")\n",
    "    \n",
    "    print(f\"🔍 Dernière version: v{latest_major}.{latest_minor}\")\n",
    "    \n",
    "    # Incrémenter la version mineure\n",
    "    next_version = f\"{latest_major}.{latest_minor + 1}\"\n",
    "    next_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(f\"🆕 Nouvelle version: v{next_version}_{next_timestamp}\")\n",
    "    \n",
    "    return next_version, next_timestamp\n",
    "\n",
    "# Test de la fonction\n",
    "next_version, next_timestamp = get_next_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ceb709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyse du modèle SavedModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 13:25:22.451643: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 13:25:22.521034: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 13:25:22.521069: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 13:25:22.522598: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 13:25:22.522624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 13:25:22.522636: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 13:25:22.650126: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 13:25:22.650177: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 13:25:22.650183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-07-16 13:25:22.650204: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-16 13:25:22.650225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13512 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:02:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modèle chargé avec succès\n",
      "📋 Signatures disponibles: ['serving_default']\n",
      "\n",
      "🔧 Analyse de la signature 'serving_default':\n",
      "   📥 Inputs:\n",
      "      └── input_tensor: (1, None, None, 3) (<dtype: 'uint8'>)\n",
      "   📤 Outputs:\n",
      "      └── raw_detection_boxes: (1, 2034, 4) (<dtype: 'float32'>)\n",
      "      └── detection_multiclass_scores: (1, 100, 3) (<dtype: 'float32'>)\n",
      "      └── detection_classes: (1, 100) (<dtype: 'float32'>)\n",
      "      └── detection_boxes: (1, 100, 4) (<dtype: 'float32'>)\n",
      "      └── raw_detection_scores: (1, 2034, 3) (<dtype: 'float32'>)\n",
      "      └── num_detections: (1,) (<dtype: 'float32'>)\n",
      "      └── detection_anchor_indices: (1, 100) (<dtype: 'float32'>)\n",
      "      └── detection_scores: (1, 100) (<dtype: 'float32'>)\n",
      "\n",
      "📏 Taille du modèle: 22.83 MB (23936569 bytes)\n"
     ]
    }
   ],
   "source": [
    "def analyze_savedmodel():\n",
    "    \"\"\"Analyse le modèle SavedModel et retourne ses informations\"\"\"\n",
    "    \n",
    "    print(\"🔍 Analyse du modèle SavedModel...\")\n",
    "    \n",
    "    # Charger le modèle\n",
    "    try:\n",
    "        model = tf.saved_model.load(TRAINING_MODEL_PATH)\n",
    "        print(\"✅ Modèle chargé avec succès\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du chargement: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Analyser les signatures\n",
    "    signatures = list(model.signatures.keys())\n",
    "    print(f\"📋 Signatures disponibles: {signatures}\")\n",
    "    \n",
    "    # Analyser la signature principale (généralement 'serving_default')\n",
    "    if 'serving_default' in signatures:\n",
    "        serving_fn = model.signatures['serving_default']\n",
    "        \n",
    "        print(\"\\n🔧 Analyse de la signature 'serving_default':\")\n",
    "        print(\"   📥 Inputs:\")\n",
    "        for name, spec in serving_fn.structured_input_signature[1].items():\n",
    "            print(f\"      └── {name}: {spec.shape} ({spec.dtype})\")\n",
    "        \n",
    "        print(\"   📤 Outputs:\")\n",
    "        for name, spec in serving_fn.structured_outputs.items():\n",
    "            print(f\"      └── {name}: {spec.shape} ({spec.dtype})\")\n",
    "    \n",
    "    # Calculer la taille du modèle\n",
    "    total_size = 0\n",
    "    for root, dirs, files in os.walk(TRAINING_MODEL_PATH):\n",
    "        for file in files:\n",
    "            total_size += os.path.getsize(os.path.join(root, file))\n",
    "    \n",
    "    size_mb = total_size / (1024 * 1024)\n",
    "    print(f\"\\n📏 Taille du modèle: {size_mb:.2f} MB ({total_size} bytes)\")\n",
    "    \n",
    "    model_info = {\n",
    "        'signatures': signatures,\n",
    "        'size_bytes': total_size,\n",
    "        'size_mb': round(size_mb, 2),\n",
    "        'architecture': 'SSD_MobileNet_V2_320x320',\n",
    "        'framework': 'TensorFlow_Object_Detection_API'\n",
    "    }\n",
    "    \n",
    "    return model_info\n",
    "\n",
    "# Analyser le modèle\n",
    "model_info = analyze_savedmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08191cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Prêt pour le déploiement!\n"
     ]
    }
   ],
   "source": [
    "def deploy_model(version, timestamp, model_info):\n",
    "    \"\"\"Déploie le modèle avec versioning automatique\"\"\"\n",
    "    \n",
    "    # Créer le nom du dossier de destination\n",
    "    version_folder = f\"v{version}_{timestamp}\"\n",
    "    destination_path = os.path.join(API_VERSIONS_PATH, version_folder)\n",
    "    \n",
    "    print(f\"🚀 Déploiement vers: {destination_path}\")\n",
    "    \n",
    "    # Créer le dossier de destination\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "    \n",
    "    # Copier le modèle SavedModel\n",
    "    model_dest_path = os.path.join(destination_path, \"saved_model\")\n",
    "    print(f\"📁 Copie du modèle SavedModel...\")\n",
    "    \n",
    "    if os.path.exists(model_dest_path):\n",
    "        shutil.rmtree(model_dest_path)\n",
    "    \n",
    "    shutil.copytree(TRAINING_MODEL_PATH, model_dest_path)\n",
    "    print(f\"✅ Modèle copié vers: {model_dest_path}\")\n",
    "    \n",
    "    # Créer les métadonnées\n",
    "    metadata = {\n",
    "        \"version\": version,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_type\": \"deep_learning_object_detection\",\n",
    "        \"deployed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"deployment_id\": timestamp,\n",
    "        \"model_size_mb\": model_info['size_mb'],\n",
    "        \"model_size_bytes\": model_info['size_bytes'],\n",
    "        \"architecture\": model_info['architecture'],\n",
    "        \"framework\": model_info['framework'],\n",
    "        \"signatures\": model_info['signatures'],\n",
    "        \"deployed_by\": \"automated_deployment_notebook\",\n",
    "        \"training_notebook\": \"dl_finetuning.ipynb\",\n",
    "        \"source_path\": TRAINING_MODEL_PATH,\n",
    "        \"deployment_type\": \"production_savedmodel\",\n",
    "        \"model_format\": \"tensorflow_savedmodel\",\n",
    "        \"deployed_path\": model_dest_path,\n",
    "        \"performance_metrics\": {\n",
    "            \"precision\": 99.8,\n",
    "            \"recall\": 96.0,\n",
    "            \"f1_score\": 97.9,\n",
    "            \"inference_time_ms\": 16,\n",
    "            \"test_images\": 394,\n",
    "            \"training_steps\": 30000\n",
    "        },\n",
    "        \"model_capabilities\": [\n",
    "            \"mushroom_detection\",\n",
    "            \"contamination_classification\", \n",
    "            \"real_time_inference\",\n",
    "            \"batch_processing\"\n",
    "        ],\n",
    "        \"input_specifications\": {\n",
    "            \"image_size\": \"320x320\",\n",
    "            \"channels\": 3,\n",
    "            \"format\": \"RGB\",\n",
    "            \"normalization\": \"0-255\"\n",
    "        },\n",
    "        \"output_specifications\": {\n",
    "            \"detection_boxes\": \"N boxes with coordinates\",\n",
    "            \"detection_classes\": \"Class IDs (1: healthy, 2: contaminated)\",\n",
    "            \"detection_scores\": \"Confidence scores 0-1\",\n",
    "            \"num_detections\": \"Number of valid detections\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Sauvegarder les métadonnées\n",
    "    metadata_path = os.path.join(destination_path, \"metadata.json\")\n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Métadonnées sauvegardées: {metadata_path}\")\n",
    "    \n",
    "    # Copier le label_map.pbtxt si disponible\n",
    "    label_map_source = \"/home/sarsator/projets/gaia_vision/training/models/dl_model/outputs/ssd_mnv2_320/label_map.pbtxt\"\n",
    "    if os.path.exists(label_map_source):\n",
    "        label_map_dest = os.path.join(destination_path, \"label_map.pbtxt\")\n",
    "        shutil.copy2(label_map_source, label_map_dest)\n",
    "        print(f\"✅ Label map copié: {label_map_dest}\")\n",
    "    \n",
    "    return destination_path, metadata\n",
    "\n",
    "print(\"🎯 Prêt pour le déploiement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3aa9e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚀 DÉMARRAGE DU DÉPLOIEMENT AUTOMATIQUE\n",
      "============================================================\n",
      "🚀 Déploiement vers: /home/sarsator/projets/gaia_vision/api/models/dl_model/versions/v1.3_20250716_132518\n",
      "📁 Copie du modèle SavedModel...\n",
      "✅ Modèle copié vers: /home/sarsator/projets/gaia_vision/api/models/dl_model/versions/v1.3_20250716_132518/saved_model\n",
      "✅ Métadonnées sauvegardées: /home/sarsator/projets/gaia_vision/api/models/dl_model/versions/v1.3_20250716_132518/metadata.json\n",
      "✅ Label map copié: /home/sarsator/projets/gaia_vision/api/models/dl_model/versions/v1.3_20250716_132518/label_map.pbtxt\n",
      "\n",
      "============================================================\n",
      "✅ DÉPLOIEMENT TERMINÉ AVEC SUCCÈS!\n",
      "============================================================\n",
      "📁 Modèle déployé dans: /home/sarsator/projets/gaia_vision/api/models/dl_model/versions/v1.3_20250716_132518\n",
      "🏷️  Version: v1.3_20250716_132518\n",
      "📏 Taille: 22.83 MB\n",
      "🏗️  Architecture: SSD_MobileNet_V2_320x320\n",
      "⚡ Performance: F1-Score 97.9% | Précision 99.8% | Rappel 96.0%\n",
      "🕐 Temps d'inférence: 16ms par image\n"
     ]
    }
   ],
   "source": [
    "# 🚀 EXÉCUTION DU DÉPLOIEMENT\n",
    "print(\"=\"*60)\n",
    "print(\"🚀 DÉMARRAGE DU DÉPLOIEMENT AUTOMATIQUE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if model_info is not None:\n",
    "    # Déployer le modèle\n",
    "    deployed_path, metadata = deploy_model(next_version, next_timestamp, model_info)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ DÉPLOIEMENT TERMINÉ AVEC SUCCÈS!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"📁 Modèle déployé dans: {deployed_path}\")\n",
    "    print(f\"🏷️  Version: v{next_version}_{next_timestamp}\")\n",
    "    print(f\"📏 Taille: {model_info['size_mb']} MB\")\n",
    "    print(f\"🏗️  Architecture: {model_info['architecture']}\")\n",
    "    print(f\"⚡ Performance: F1-Score 97.9% | Précision 99.8% | Rappel 96.0%\")\n",
    "    print(f\"🕐 Temps d'inférence: 16ms par image\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Échec de l'analyse du modèle - Déploiement annulé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e593e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 VALIDATION DU DÉPLOIEMENT\n",
      "----------------------------------------\n",
      "   ✅ Dossier de déploiement créé\n",
      "   ✅ SavedModel présent\n",
      "   ✅ saved_model.pb présent\n",
      "   ✅ variables présent\n",
      "   ✅ Métadonnées présentes\n",
      "   ✅ Métadonnées valides\n",
      "   ✅ Label map présente\n",
      "   ✅ Modèle chargeable\n",
      "   ✅ Signature d'inférence disponible\n",
      "\n",
      "📊 Résultat: 9/9 vérifications réussies\n",
      "🎉 VALIDATION RÉUSSIE - Modèle prêt pour l'utilisation!\n"
     ]
    }
   ],
   "source": [
    "def validate_deployment(deployed_path):\n",
    "    \"\"\"Valide que le déploiement s'est bien déroulé\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 VALIDATION DU DÉPLOIEMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # Vérifier que le dossier existe\n",
    "    if os.path.exists(deployed_path):\n",
    "        checks.append(\"✅ Dossier de déploiement créé\")\n",
    "    else:\n",
    "        checks.append(\"❌ Dossier de déploiement manquant\")\n",
    "        return False\n",
    "    \n",
    "    # Vérifier le modèle SavedModel\n",
    "    saved_model_path = os.path.join(deployed_path, \"saved_model\")\n",
    "    if os.path.exists(saved_model_path):\n",
    "        checks.append(\"✅ SavedModel présent\")\n",
    "        \n",
    "        # Vérifier les fichiers critiques\n",
    "        critical_files = [\"saved_model.pb\", \"variables\"]\n",
    "        for file in critical_files:\n",
    "            file_path = os.path.join(saved_model_path, file)\n",
    "            if os.path.exists(file_path):\n",
    "                checks.append(f\"✅ {file} présent\")\n",
    "            else:\n",
    "                checks.append(f\"❌ {file} manquant\")\n",
    "    else:\n",
    "        checks.append(\"❌ SavedModel manquant\")\n",
    "    \n",
    "    # Vérifier les métadonnées\n",
    "    metadata_path = os.path.join(deployed_path, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        checks.append(\"✅ Métadonnées présentes\")\n",
    "        \n",
    "        # Tester le chargement du JSON\n",
    "        try:\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            checks.append(\"✅ Métadonnées valides\")\n",
    "        except:\n",
    "            checks.append(\"❌ Métadonnées corrompues\")\n",
    "    else:\n",
    "        checks.append(\"❌ Métadonnées manquantes\")\n",
    "    \n",
    "    # Vérifier la label map\n",
    "    label_map_path = os.path.join(deployed_path, \"label_map.pbtxt\")\n",
    "    if os.path.exists(label_map_path):\n",
    "        checks.append(\"✅ Label map présente\")\n",
    "    else:\n",
    "        checks.append(\"⚠️  Label map manquante (optionnelle)\")\n",
    "    \n",
    "    # Test de chargement du modèle\n",
    "    try:\n",
    "        test_model = tf.saved_model.load(saved_model_path)\n",
    "        checks.append(\"✅ Modèle chargeable\")\n",
    "        \n",
    "        # Test d'inférence rapide\n",
    "        if 'serving_default' in test_model.signatures:\n",
    "            checks.append(\"✅ Signature d'inférence disponible\")\n",
    "        else:\n",
    "            checks.append(\"⚠️  Signature 'serving_default' non trouvée\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        checks.append(f\"❌ Erreur de chargement: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    for check in checks:\n",
    "        print(f\"   {check}\")\n",
    "    \n",
    "    # Compter les succès\n",
    "    success_count = sum(1 for check in checks if check.startswith(\"✅\"))\n",
    "    total_count = len([c for c in checks if not c.startswith(\"⚠️\")])\n",
    "    \n",
    "    print(f\"\\n📊 Résultat: {success_count}/{total_count} vérifications réussies\")\n",
    "    \n",
    "    if success_count >= total_count * 0.8:  # 80% de réussite minimum\n",
    "        print(\"🎉 VALIDATION RÉUSSIE - Modèle prêt pour l'utilisation!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ VALIDATION ÉCHOUÉE - Vérifier les erreurs ci-dessus\")\n",
    "        return False\n",
    "\n",
    "# Exécuter la validation si le déploiement a réussi\n",
    "if 'deployed_path' in locals():\n",
    "    validation_success = validate_deployment(deployed_path)\n",
    "else:\n",
    "    print(\"⚠️  Aucun déploiement à valider\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
